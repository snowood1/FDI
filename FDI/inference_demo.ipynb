{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eeed760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T06:30:34.147447Z",
     "start_time": "2022-10-21T06:30:00.434623Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_tokens to be addded to GPT2 tokenizer: \n",
      "{'<|startofinfill|>': 50257, '<|endofinfill|>': 50258, '<|infill_document|>': 50259, '<|infill_paragraph|>': 50260, '<|infill_sentence|>': 50261, '<|infill_ngram|>': 50262, '<|infill_word|>': 50263}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from transformers import GPT2LMHeadModel\n",
    "from inference import get_tokenizer,get_masks,infill_with_ilm,infill_with_naive,get_additional_id\n",
    "from helper import get_color_fonts,apply_masked_spans_with_color,remove_color_fonts\n",
    "\n",
    "additional_tokens_to_ids = get_additional_id('../ILM/additional_ids_to_tokens.pkl')\n",
    "tokenizer = get_tokenizer('gpt2',additional_tokens_to_ids)\n",
    "true_color = get_color_fonts(colors='BackgroundCyan', tokenizer = tokenizer)\n",
    "fake_color = get_color_fonts(colors='BackgroundRed', tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f039618",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ada534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T06:30:34.291121Z",
     "start_time": "2022-10-21T06:30:34.150222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3482\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Original text\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For instance, downstream harms to particular groups are often blamed on \"biased data,\" but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into five distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general claims about what may or may not be \"fair.\"\n",
      "\n",
      "Total number of the sentences:  6\n",
      "Total token number of the documnet:  163\n",
      "\n",
      "Extracted important phrases from rake:\n",
      " [('machine learning increasingly affects people', 21.0), ('distinct categories spanning', 9.0), ('machine learning pipeline', 9.0), ('unwanted consequences arise', 8.5), ('understanding unintended consequences', 8.0), ('data generation processes', 7.833333333333334), ('machine learning', 6.0), ('data generation', 4.833333333333334), ('biased data', 4.333333333333334), ('unified understanding', 4.0), ('downstream harms', 4.0), ('concept encompass', 4.0), ('partitions sources', 4.0), ('downstream harm', 4.0), ('issues arise', 4.0), ('application-specific populations', 4.0), ('general claims', 4.0), ('developing solutions', 3.333333333333333), ('understanding', 2.0), ('issues', 1.5), ('solutions', 1.3333333333333333), ('framework', 1.0), ('society', 1.0), ('important', 1.0), ('strive', 1.0), ('comprehensive', 1.0), ('instance', 1.0), ('groups', 1.0), ('blamed', 1.0), ('paper', 1.0), ('provide', 1.0), ('describe', 1.0), ('relevant', 1.0), ('applications', 1.0), ('motivate', 1.0), ('aim', 1.0), ('facilitate', 1.0), ('development', 1.0), ('stem', 1.0), ('relying', 1.0), ('fair', 1.0)]\n",
      "\n",
      "\n",
      "------ generating the 0 th example ---\n",
      "\toverlapping: machine learning pipeline range(588, 613)\n",
      "\toverlapping: data generation processes range(879, 904)\n",
      "\toverlapping: downstream harm range(489, 504)\n",
      "\toverlapping: general claims range(929, 943)\n",
      "instance downstream harms\n",
      "partitions sources downstream harm\n",
      "aim facilitate\n",
      "aim to facilitate the development\n",
      "aim to facilitate the development solutions\n",
      "an understanding application-specific populations\n",
      "an understanding of application-specific populations data generation processes\n",
      "relying general claims\n",
      "\n",
      "mask:\t [(<MaskHierarchicalType.NGRAM: 3>, 260, 26), (<MaskHierarchicalType.NGRAM: 3>, 347, 22), (<MaskHierarchicalType.NGRAM: 3>, 379, 6), (<MaskHierarchicalType.NGRAM: 3>, 402, 20), (<MaskHierarchicalType.NGRAM: 3>, 467, 37), (<MaskHierarchicalType.NGRAM: 3>, 535, 28), (<MaskHierarchicalType.NGRAM: 3>, 588, 25), (<MaskHierarchicalType.WORD: 4>, 618, 8), (<MaskHierarchicalType.NGRAM: 3>, 664, 8), (<MaskHierarchicalType.NGRAM: 3>, 760, 46), (<MaskHierarchicalType.NGRAM: 3>, 822, 82), (<MaskHierarchicalType.NGRAM: 3>, 918, 25)]\n",
      "masked rate(token):\t 0.20245398773006135\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Original text\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not accompanied by a schema or by a valid schema. Relax NG is a popular and powerful schema language, which supports the unconstrained interleaving operator. Focusing on the inference of Relax NG, we propose a new subclass of regular expressions with interleaving and design a polynomial inference algorithm. Then we conducted a series of experiments based on large-scale real data and on three XML data corpora, and experimental results show that our subclass has a better practicality than previous ones, and the regular expressions inferred by our algorithm are more precise.\n",
      "\n",
      "Total number of the sentences:  6\n",
      "Total token number of the documnet:  128\n",
      "\n",
      "Extracted important phrases from rake:\n",
      " [('learning restricted regular expressions', 14.0), ('large-scale real data', 9.0), ('experimental results show', 9.0), ('regular expressions inferred', 9.0), ('xml data corpora', 8.25), ('powerful schema language', 8.0), ('unconstrained interleaving operator', 7.666666666666667), ('polynomial inference algorithm', 7.0), ('regular expressions', 6.0), ('xml schema', 4.25), ('xml documents', 4.25), ('valid schema', 4.0), ('schema', 2.0), ('inference', 2.0), ('algorithm', 2.0), ('interleaving', 1.6666666666666667), ('advantages', 1.0), ('presence', 1.0), ('numerous', 1.0), ('practice', 1.0), ('accompanied', 1.0), ('relax', 1.0), ('popular', 1.0), ('supports', 1.0), ('focusing', 1.0), ('propose', 1.0), ('subclass', 1.0), ('design', 1.0), ('conducted', 1.0), ('series', 1.0), ('experiments', 1.0), ('practicality', 1.0), ('previous', 1.0), ('precise', 1.0)]\n",
      "\n",
      "\n",
      "------ generating the 0 th example ---\n",
      "\toverlapping: powerful schema language range(259, 283)\n",
      "\toverlapping: the inference range(353, 366)\n",
      "\toverlapping: accompanied range(187, 198)\n",
      "\toverlapping: a popular range(245, 254)\n",
      "\toverlapping: relax range(233, 238)\n",
      "accompanied a schema\n",
      "a popular powerful schema language\n",
      "focusing the inference\n",
      "practicality previous\n",
      "\n",
      "mask:\t [(<MaskHierarchicalType.NGRAM: 3>, 187, 23), (<MaskHierarchicalType.NGRAM: 3>, 233, 5), (<MaskHierarchicalType.NGRAM: 3>, 245, 38), (<MaskHierarchicalType.NGRAM: 3>, 341, 25), (<MaskHierarchicalType.WORD: 4>, 397, 8), (<MaskHierarchicalType.NGRAM: 3>, 451, 6), (<MaskHierarchicalType.WORD: 4>, 500, 9), (<MaskHierarchicalType.NGRAM: 3>, 543, 21), (<MaskHierarchicalType.NGRAM: 3>, 600, 25), (<MaskHierarchicalType.NGRAM: 3>, 657, 26), (<MaskHierarchicalType.NGRAM: 3>, 694, 32), (<MaskHierarchicalType.NGRAM: 3>, 734, 9), (<MaskHierarchicalType.WORD: 4>, 753, 7)]\n",
      "masked rate(token):\t 0.1953125\n"
     ]
    }
   ],
   "source": [
    "with open('../data/raw_data/cs_clean/test.txt', 'r') as f:\n",
    "    raw = f.read().split('\\n\\n\\n')\n",
    "print(len(raw))\n",
    "\n",
    "df = pd.DataFrame(raw, columns = ['doc'])\n",
    "# df['words'] =  df['doc'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "# df1 = df[((df.words>180) & (df.words<200))] \n",
    "# Idx = sorted(df1.sample(n=5).index)\n",
    "\n",
    "Idx = [3005,3206]\n",
    "\n",
    "\n",
    "original_doc_list = df.doc[Idx]\n",
    "masked_data = get_masks(original_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2921119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T06:30:34.356862Z",
     "start_time": "2022-10-21T06:30:34.293266Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "---------------------------------------- 0 3005 ----------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For \u001b[46minstance, downstream harms\u001b[0m to particular groups are often blamed on \"biased data,\" but \u001b[46mthis concept encompass\u001b[0m too many \u001b[46missues\u001b[0m to be useful in \u001b[46mdeveloping solutions\u001b[0m. In this paper, we provide a framework that \u001b[46mpartitions sources of downstream harm\u001b[0m in machine learning into five \u001b[46mdistinct categories spanning\u001b[0m the data generation and \u001b[46mmachine learning pipeline\u001b[0m. We \u001b[46mdescribe\u001b[0m how these issues arise, how they are \u001b[46mrelevant\u001b[0m to particular applications, and how they motivate different solutions. In doing so, we \u001b[46maim to facilitate the development of solutions\u001b[0m that stem from \u001b[46man understanding of application-specific populations and data generation processes\u001b[0m, rather than \u001b[46mrelying on general claims\u001b[0m about what may or may not be \"fair.\"\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------------------- 1 3206 ----------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not \u001b[46maccompanied by a schema\u001b[0m or by a valid schema. \u001b[46mRelax\u001b[0m NG is \u001b[46ma popular and powerful schema language\u001b[0m, which supports the unconstrained interleaving operator. \u001b[46mFocusing on the inference\u001b[0m of Relax NG, we propose a new \u001b[46msubclass\u001b[0m of regular expressions with interleaving and \u001b[46mdesign\u001b[0m a polynomial inference algorithm. Then we \u001b[46mconducted\u001b[0m a series of experiments based on \u001b[46mlarge-scale real data\u001b[0m and on three XML data corpora, and \u001b[46mexperimental results show\u001b[0m that our subclass has a better \u001b[46mpracticality than previous\u001b[0m ones, and \u001b[46mthe regular expressions inferred\u001b[0m by our \u001b[46malgorithm\u001b[0m are more \u001b[46mprecise\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "for doc_id, (real_id, (doc, examples)) in enumerate(zip(Idx, masked_data)):\n",
    "    if len(examples) == 0:\n",
    "        continue\n",
    "    masked_spans = random.choice(examples)\n",
    "    mask_span_type_to_str = {t: '<|infill_{}|>'.format(str(t).split(\".\")[1].lower()) for t, _, _ in masked_spans}\n",
    "    context, answers, color_text = apply_masked_spans_with_color(\n",
    "        doc,\n",
    "        masked_spans,\n",
    "        mask_span_type_to_str, true_color)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('-' * 40,doc_id, real_id,  '-'* 40 )\n",
    "    print('-' * 80)\n",
    "    print(' ' * 36 + 'ORIGINAL')\n",
    "    print(color_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0defa9",
   "metadata": {},
   "source": [
    "# FDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46defbb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T06:30:52.088274Z",
     "start_time": "2022-10-21T06:30:34.359825Z"
    },
    "code_folding": [
     16
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/cs_clean_ilm\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------------------- 0 3005 ----------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For \u001b[46minstance, downstream harms\u001b[0m to particular groups are often blamed on \"biased data,\" but \u001b[46mthis concept encompass\u001b[0m too many \u001b[46missues\u001b[0m to be useful in \u001b[46mdeveloping solutions\u001b[0m. In this paper, we provide a framework that \u001b[46mpartitions sources of downstream harm\u001b[0m in machine learning into five \u001b[46mdistinct categories spanning\u001b[0m the data generation and \u001b[46mmachine learning pipeline\u001b[0m. We \u001b[46mdescribe\u001b[0m how these issues arise, how they are \u001b[46mrelevant\u001b[0m to particular applications, and how they motivate different solutions. In doing so, we \u001b[46maim to facilitate the development of solutions\u001b[0m that stem from \u001b[46man understanding of application-specific populations and data generation processes\u001b[0m, rather than \u001b[46mrelying on general claims\u001b[0m about what may or may not be \"fair.\"\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For \u001b[41m example, negative sentences\u001b[0m to particular groups are often blamed on \"biased data,\" but \u001b[41m do we know\u001b[0m too many \u001b[41m statistical mistakes\u001b[0m to be useful in \u001b[41m understanding the impact these negative sentences have\u001b[0m. In this paper, we provide a framework that \u001b[41m delineates the relevant issues\u001b[0m in machine learning into five \u001b[41m major categories: causality, category, context,\u001b[0m the data generation and \u001b[41m fixation\u001b[0m. We \u001b[41m study\u001b[0m how these issues arise, how they are \u001b[41m intertwined\u001b[0m to particular applications, and how they motivate different solutions. In doing so, we \u001b[41m then study four newly introduced concepts\u001b[0m that stem from \u001b[41m corpora from the genres of diachronic linguistics\u001b[0m, rather than \u001b[41m reacting to negative sentences\u001b[0m about what may or may not be \"fair.\"\n",
      "----------------------------------------\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For example, negative sentences to particular groups are often blamed on \"biased data,\" but do we know too many statistical mistakes to be useful in understanding the impact these negative sentences have . In this paper, we provide a framework that delineates the relevant issues in machine learning into five major categories: causality, category, context, the data generation and fixation . We study how these issues arise, how they are intertwined to particular applications, and how they motivate different solutions. In doing so, we then study four newly introduced concepts that stem from corpora from the genres of diachronic linguistics , rather than reacting to negative sentences about what may or may not be \"fair.\"\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For \u001b[41m example, theoretical \"data bias\"\u001b[0m to particular groups are often blamed on \"biased data,\" but \u001b[41m indeed the number of errors in\u001b[0m too many \u001b[41m data might not always be sufficiently large\u001b[0m to be useful in \u001b[41m practice\u001b[0m. In this paper, we provide a framework that \u001b[41m addresses those issues in two ways:\u001b[0m in machine learning into five \u001b[41m groups as opposed to other factors limiting\u001b[0m the data generation and \u001b[41m may motivate them\u001b[0m. We \u001b[41m discover\u001b[0m how these issues arise, how they are \u001b[41m persistent, the impact that machine learning might have\u001b[0m to particular applications, and how they motivate different solutions. In doing so, we \u001b[41m then argue\u001b[0m that stem from \u001b[41m the recent publication of a leading unsupervised solution\u001b[0m, rather than \u001b[41m discouraging predictions about the behavior of machines\u001b[0m about what may or may not be \"fair.\"\n",
      "----------------------------------------\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For example, theoretical \"data bias\" to particular groups are often blamed on \"biased data,\" but indeed the number of errors in too many data might not always be sufficiently large to be useful in practice . In this paper, we provide a framework that addresses those issues in two ways: in machine learning into five groups as opposed to other factors limiting the data generation and may motivate them . We discover how these issues arise, how they are persistent, the impact that machine learning might have to particular applications, and how they motivate different solutions. In doing so, we then argue that stem from the recent publication of a leading unsupervised solution , rather than discouraging predictions about the behavior of machines about what may or may not be \"fair.\"\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For \u001b[41m example, how ethical concerns\u001b[0m to particular groups are often blamed on \"biased data,\" but \u001b[41m should we take steps to remove\u001b[0m too many \u001b[41m bias\u001b[0m to be useful in \u001b[41m general\u001b[0m. In this paper, we provide a framework that \u001b[41m represents a multidisciplinary effort\u001b[0m in machine learning into five \u001b[41m different notions of fairness, including de facto\u001b[0m the data generation and \u001b[41m data generation approaches\u001b[0m. We \u001b[41m address\u001b[0m how these issues arise, how they are \u001b[41m resolved\u001b[0m to particular applications, and how they motivate different solutions. In doing so, we \u001b[41m also find a number of well-known\u001b[0m that stem from \u001b[41m a common-sense approach\u001b[0m, rather than \u001b[41m iterating parameters and choosing among them\u001b[0m about what may or may not be \"fair.\"\n",
      "----------------------------------------\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For example, how ethical concerns to particular groups are often blamed on \"biased data,\" but should we take steps to remove too many bias to be useful in general . In this paper, we provide a framework that represents a multidisciplinary effort in machine learning into five different notions of fairness, including de facto the data generation and data generation approaches . We address how these issues arise, how they are resolved to particular applications, and how they motivate different solutions. In doing so, we also find a number of well-known that stem from a common-sense approach , rather than iterating parameters and choosing among them about what may or may not be \"fair.\"\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------------------- 1 3206 ----------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not \u001b[46maccompanied by a schema\u001b[0m or by a valid schema. \u001b[46mRelax\u001b[0m NG is \u001b[46ma popular and powerful schema language\u001b[0m, which supports the unconstrained interleaving operator. \u001b[46mFocusing on the inference\u001b[0m of Relax NG, we propose a new \u001b[46msubclass\u001b[0m of regular expressions with interleaving and \u001b[46mdesign\u001b[0m a polynomial inference algorithm. Then we \u001b[46mconducted\u001b[0m a series of experiments based on \u001b[46mlarge-scale real data\u001b[0m and on three XML data corpora, and \u001b[46mexperimental results show\u001b[0m that our subclass has a better \u001b[46mpracticality than previous\u001b[0m ones, and \u001b[46mthe regular expressions inferred\u001b[0m by our \u001b[46malgorithm\u001b[0m are more \u001b[46mprecise\u001b[0m.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not \u001b[41m comprehensible by default or by distance-restricted queries\u001b[0m or by a valid schema. \u001b[41m Although the Relax\u001b[0m NG is \u001b[41mo appropriate for NG\u001b[0m, which supports the unconstrained interleaving operator. \u001b[41m First, by leveraging the power\u001b[0m of Relax NG, we propose a new \u001b[41m class\u001b[0m of regular expressions with interleaving and \u001b[41m interpolation consistency by\u001b[0m a polynomial inference algorithm. Then we \u001b[41m propose\u001b[0m a series of experiments based on \u001b[41m standard relaxed encoding techniques\u001b[0m and on three XML data corpora, and \u001b[41m of experiments involving real-world data\u001b[0m that our subclass has a better \u001b[41m compact and predictable regular expression\u001b[0m ones, and \u001b[41m robust to imprecision, explainability, simplifyability\u001b[0m by our \u001b[41m thus the subclass is more efficient and\u001b[0m are more \u001b[41m reasonable\u001b[0m.\n",
      "----------------------------------------\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not comprehensible by default or by distance-restricted queries or by a valid schema. Although the Relax NG is o appropriate for NG , which supports the unconstrained interleaving operator. First, by leveraging the power of Relax NG, we propose a new class of regular expressions with interleaving and interpolation consistency by a polynomial inference algorithm. Then we propose a series of experiments based on standard relaxed encoding techniques and on three XML data corpora, and of experiments involving real-world data that our subclass has a better compact and predictable regular expression ones, and robust to imprecision, explainability, simplifyability by our thus the subclass is more efficient and are more reasonable .\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not \u001b[41m including valid scratch corpora, schema\u001b[0m or by a valid schema. \u001b[41m In this paper\u001b[0m NG is \u001b[41m not compact\u001b[0m, which supports the unconstrained interleaving operator. \u001b[41m In order to distinguish between  strong assertions\u001b[0m of Relax NG, we propose a new \u001b[41m class\u001b[0m of regular expressions with interleaving and \u001b[41m search procedures inspired by\u001b[0m a polynomial inference algorithm. Then we \u001b[41m propose\u001b[0m a series of experiments based on \u001b[41m the temporal logic present in natural languages\u001b[0m and on three XML data corpora, and \u001b[41m our show\u001b[0m that our subclass has a better \u001b[41m expressive power than the extensible few existing\u001b[0m ones, and \u001b[41m our framework can be easily adapted to other\u001b[0m by our \u001b[41m subclass\u001b[0m are more \u001b[41m refined\u001b[0m.\n",
      "----------------------------------------\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not including valid scratch corpora, schema or by a valid schema. In this paper NG is not compact , which supports the unconstrained interleaving operator. In order to distinguish between  strong assertions of Relax NG, we propose a new class of regular expressions with interleaving and search procedures inspired by a polynomial inference algorithm. Then we propose a series of experiments based on the temporal logic present in natural languages and on three XML data corpora, and our show that our subclass has a better expressive power than the extensible few existing ones, and our framework can be easily adapted to other by our subclass are more refined .\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not \u001b[41m LO in general, either\u001b[0m or by a valid schema. \u001b[41m These can be solved by\u001b[0m NG is \u001b[41m better than Relax NG\u001b[0m, which supports the unconstrained interleaving operator. \u001b[41m To achieve an an improvement\u001b[0m of Relax NG, we propose a new \u001b[41m subclass\u001b[0m of regular expressions with interleaving and \u001b[41m independently enumerated and propose\u001b[0m a polynomial inference algorithm. Then we \u001b[41m present\u001b[0m a series of experiments based on \u001b[41m empirical evaluations\u001b[0m and on three XML data corpora, and \u001b[41m compare  O(log n)\u001b[0m that our subclass has a better \u001b[41m practice than the standard IR-L\u001b[0m ones, and \u001b[41m Fierstein bounds in general\u001b[0m by our \u001b[41m relaxation of regular expressions\u001b[0m are more \u001b[41m efficient\u001b[0m.\n",
      "----------------------------------------\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not LO in general, either or by a valid schema. These can be solved by NG is better than Relax NG , which supports the unconstrained interleaving operator. To achieve an an improvement of Relax NG, we propose a new subclass of regular expressions with interleaving and independently enumerated and propose a polynomial inference algorithm. Then we present a series of experiments based on empirical evaluations and on three XML data corpora, and compare  O(log n) that our subclass has a better practice than the standard IR-L ones, and Fierstein bounds in general by our relaxation of regular expressions are more efficient .\n"
     ]
    }
   ],
   "source": [
    "TASK = 'ilm'\n",
    "DATASET = 'cs_clean'\n",
    "MAX_SEQ_LEN  = 512\n",
    "\n",
    "NUM_FAKE = 3\n",
    "MODEL_DIR = '../model/%s_%s'%(DATASET, TASK)\n",
    "print(MODEL_DIR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "_ = model.to(device)\n",
    "\n",
    "\n",
    "penalty = 1.2\n",
    "\n",
    "for doc_id, (real_id, (doc, examples)) in enumerate(zip(Idx, masked_data)):\n",
    "    if len(examples) == 0:\n",
    "        continue\n",
    "    masked_spans = random.choice(examples)\n",
    "    mask_span_type_to_str = {t: '<|infill_{}|>'.format(str(t).split(\".\")[1].lower()) for t, _, _ in masked_spans}\n",
    "    context, answers, color_text = apply_masked_spans_with_color(\n",
    "        doc,\n",
    "        masked_spans,\n",
    "        mask_span_type_to_str, true_color)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('-' * 40,doc_id, real_id,  '-'* 40 )\n",
    "    print('-' * 80)\n",
    "    print(' ' * 36 + 'ORIGINAL')\n",
    "    print(color_text)\n",
    "        \n",
    "    \n",
    "#     context_ids = tokenizer.encode(context)\n",
    "    context1  = context.replace('<|infill_ngram|>', '________').replace('<|infill_word|>', '________').replace('<|infill_sentence|>', '________')\n",
    "    context_ids = tokenizer.encode(context1)\n",
    "    _blank_id = tokenizer.encode('________')[0]\n",
    "    for i in masked_spans:\n",
    "        keys =  mask_span_type_to_str[i[0]]\n",
    "        context_ids[context_ids.index(_blank_id)] = tokenizer.encode(keys)[0]\n",
    "    \n",
    "    generated = infill_with_ilm(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    additional_tokens_to_ids,\n",
    "    context_ids,\n",
    "    num_infills=NUM_FAKE,\n",
    "    max_sequence_length=MAX_SEQ_LEN,\n",
    "    temp=1,\n",
    "    topk=None,\n",
    "    nucleus=0.95,\n",
    "    add_colors=fake_color,\n",
    "    penalty=penalty,\n",
    "    answers=answers)\n",
    "    \n",
    "    generated_samples = [] \n",
    "    \n",
    "    for idx, g in enumerate(generated):\n",
    "        print('-' * 80)\n",
    "        print(' ' * 36 + 'FAKE ' + str(idx))\n",
    "        fake_with_color = tokenizer.decode(g)\n",
    "        print(fake_with_color)\n",
    "        print(\"-\" *40)\n",
    "        fake_no_color = remove_color_fonts(fake_with_color,fake_color)\n",
    "        print(fake_no_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfe97b",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c1f0a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T06:31:06.211796Z",
     "start_time": "2022-10-21T06:30:52.090735Z"
    },
    "code_folding": [
     13
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/cs_clean_lm\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------------------- 0 ----------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For instance, downstream harms to particular groups are often blamed on \"biased data,\" but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into five distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general claims about what may or may not be \"fair.\"\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. Machine learning advances; some of these advances are discussed in this paper. In light of this fact, this paper introduces a general framework for understanding unintended consequences. The framework defines four possible handling models, and the four handling models are advanced, unsound, intuitive, explanatory, and meaningfully driven models. The framework is used to model the effects of new learning paradigms on existing unintended predictions of machine learning systems and its implementation.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. Machine Learning. d Cellular devices have been implicated as a cause of unnecessary deaths for many reasons. A challenge is tackling the unintended consequences of machine learning---whether unintentional or unintentional-probabilistic. Without a proper understanding, however, if one can not identify the unintended consequences of machine learning, several more subtle consequences can arise. This paper presents a formal framework for understanding unintended consequences of machine learning. The framework describes tools for understanding unintended consequences from the perspective of machine learning and introduces three kinds of tools. We present the three kinds of tools and show by experimental results that the problems with the considered tools have been taken into account and difficult cases for domain experts to identify can be prevented.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "A Framework for Understanding Unintended Consequences of Machine Learning\n",
      "As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. Machine Learning is changing from a pattern recognition method with a built-in implicit memory to an application of modern deep learning methods for learning new patterns. Although deep learning has shown promising results, it is still not specified the criteria that will determine when they will succeed or fail. This paper addresses this problem by modeling unintended consequences using two basic models, assuming that the current large-scale dataset is sampled from unlabeled samples from different segments (TSP and SSM). We provide an analysis of both the empirical results, which are consistent with the important properties of these models, and also the results obtained based on the probabilistic modeling framework.\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------------------- 1 ----------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. However, many XML documents in practice are not accompanied by a schema or by a valid schema. Relax NG is a popular and powerful schema language, which supports the unconstrained interleaving operator. Focusing on the inference of Relax NG, we propose a new subclass of regular expressions with interleaving and design a polynomial inference algorithm. Then we conducted a series of experiments based on large-scale real data and on three XML data corpora, and experimental results show that our subclass has a better practicality than previous ones, and the regular expressions inferred by our algorithm are more precise.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. O(1) uses space for storing documents, representing the likely values of attributes and the constraints associated with their assignments and persistence, saving storing dynamically the exact values of the documents from XML. The Satisfiability Modulo Theories system presents a simple relation between the schema and the relational database, leveraging this modality to express constraints. It can be analyzed as a partial QRefinement. We evaluate this relation on five industrial data sets of XML query processing, including one from Thorup, noting, against experimental results, that when coupled with encodings based on satisfiability modulo theories, the satisfiability modulo theories encoding a schema can be evaluated in linear time, comparing to O(log n) query time.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. -Approaches such as the XML Schema Library and the associated XML Schema DSL (XSD) are promising tools for such topics. However, in our previous researches, they mainly assume that operators on XML schemas and applications are defined and modeled through XML fragments. We aim to close this gap by emphasizing the presence of a few restricted regular expressions on these fragments. For the specific case of XPath schemas, we present two alternatives, namely those based on gap rules, and those based on joins, that have advantages over the former. We then present theoretical results about the rules used for these approaches and their flexibility in defining rules. Finally, we perform some numerical experiments and state the preliminary result.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Learning Restricted Regular Expressions with Interleaving\n",
      "The advantages for the presence of an XML schema for XML documents are numerous. Semantic web technologies validate information in XML which exhibit, under certain conditions, semantics approximations provided by each XML component. We will illustrate this by proposing a sequence of \"sub-expression\" fragments of a restricted regular expression which preserves semantics with respect to a certain relationship relation that a permitted entity in the data relationship exhibits. This relation can be either an attribute assignment, extension or blacklist. The required syntactic constructs are as follows: In parenthesis, block constructor and binding. On one hand, structural inspection of the extracted information may make the whole structure of the constructed relation and constructions occur more concisely, on the other hand it can reduce the analysis of the meta data, hindering typical cluster analysis. This application of restricted regular expressions also extends the applicability of analysis in query answering, in particular to XML document classification.\n"
     ]
    }
   ],
   "source": [
    "TASK = 'lm'\n",
    "DATASET = 'cs_clean'\n",
    "MAX_SEQ_LEN  = 512\n",
    "NUM_FAKE = 3\n",
    "MODEL_DIR = '../model/%s_%s'%(DATASET, TASK)\n",
    "print(MODEL_DIR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "_ = model.to(device)\n",
    "\n",
    "\n",
    "for doc_id, (doc, examples) in enumerate(masked_data):\n",
    "    if len(examples) == 0:\n",
    "        continue\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('-' * 40,doc_id, '-'* 40 )\n",
    "    print('-' * 80)\n",
    "    print(' ' * 36 + 'ORIGINAL')\n",
    "    print(doc)\n",
    "        \n",
    "    prompt = doc.split('\\n')[0]+ '\\n'+ nltk.sent_tokenize(doc.split('\\n')[1])[0]\n",
    "    # prompt = doc.split('\\n')[0] +'\\n'\n",
    "    context_ids = tokenizer.encode(prompt)\n",
    "\n",
    "    generated = infill_with_naive(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    additional_tokens_to_ids,\n",
    "    context_ids,\n",
    "    num_infills=NUM_FAKE *2,\n",
    "    max_sequence_length=MAX_SEQ_LEN,\n",
    "    temp=1,\n",
    "    topk=None,\n",
    "    nucleus=0.95)\n",
    "    generated_samples = [] \n",
    "    \n",
    "    \n",
    "    # GPT-2 can't control the lengths, so we can select the outputs with the lengths closest to the original stories.\n",
    "    \n",
    "    original_len = len(tokenizer.encode(doc))\n",
    "    diff = np.array([abs(len(g)-original_len) for g in generated])\n",
    "    smallest = np.argpartition(diff,NUM_FAKE)[:NUM_FAKE]\n",
    "    \n",
    "    np.random.shuffle(smallest)\n",
    "    for idx, i in enumerate(smallest):\n",
    "        g = generated[i]\n",
    "        print('-' * 80)\n",
    "        print(' ' * 36 + 'FAKE ' + str(idx))\n",
    "        tmp = tokenizer.decode(g)\n",
    "        tmp = prompt +  ' ' + tmp.strip()[:1].upper() + tmp.strip()[1:]\n",
    "        print(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
