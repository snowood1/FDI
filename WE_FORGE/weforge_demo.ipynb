{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f2a8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T02:54:15.316049Z",
     "start_time": "2022-10-21T02:54:13.654428Z"
    },
    "code_folding": [
     20,
     40,
     66,
     78,
     94,
     102,
     108,
     128,
     212,
     235
    ]
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from math import floor\n",
    "\n",
    "# Auxiliary Funcs\n",
    "_PUNC_ = string.punctuation\n",
    "CGREEN =\"\\033[46m\"\n",
    "CEND = '\\033[0m'\n",
    "CRED = \"\\033[41m\"\n",
    "\n",
    "def line_process(text, tag=False, remove_punc=True, lower=True):\n",
    "    \"\"\"\n",
    "    Auxiliary function that preprocess the given line\n",
    "    lowercase - strip - remove punc - tokenize - (pos_tag)\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    if remove_punc:\n",
    "        text = \"\".join([char for char in text if char not in _PUNC_])\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    if lower:\n",
    "        text = [item.lower() for item in text]\n",
    "\n",
    "    text_tags = None\n",
    "    if tag:\n",
    "        text_tags = pos_tag(text)\n",
    "\n",
    "    return text, text_tags\n",
    "\n",
    "def file2corpus(file_path, corpus=None, with_tag=False, with_punc=False, lower=True):\n",
    "    \"\"\"\n",
    "    Auxiliary function that read document from a file and write them into a corpus (list_of_docs)\n",
    "    \"\"\"\n",
    "    corpus = corpus if corpus is not None else list()\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    new_doc = False\n",
    "    for eachLine in lines:\n",
    "        if eachLine == '\\n':\n",
    "            new_doc = False\n",
    "        else:\n",
    "            if with_tag:\n",
    "                _, lineComp = line_process(eachLine, tag=True, remove_punc=operator.not_(with_punc), lower=lower)\n",
    "            else:\n",
    "                lineComp, _ = line_process(eachLine, tag=False, remove_punc=operator.not_(with_punc), lower=lower)\n",
    "            if not new_doc:\n",
    "                new_doc = True\n",
    "                corpus.append(lineComp)\n",
    "                corpus[-1].append('\\n')\n",
    "            else:\n",
    "                corpus[-1] += lineComp\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def concatTokens(token_list):\n",
    "    \"\"\"\n",
    "    Auxiliary function that print the token in a human-friendly fashion\n",
    "    \"\"\"\n",
    "    ans = list()\n",
    "    for eachToken in token_list:\n",
    "        if eachToken not in _PUNC_:\n",
    "            ans.append(\" \")\n",
    "        ans.append(eachToken)\n",
    "\n",
    "    return \"\".join(ans[1:])\n",
    "\n",
    "def model_test(corpus, capital_corpus, w2m, c2w, conf):\n",
    "    \"\"\"\n",
    "    Take original documents, meta and configurations, generate faked doc tokens\n",
    "    \"\"\"\n",
    "    num_bin, tgt_bin, num_rep = conf.num_bin, conf.tgt_bin, conf.num_rep\n",
    "\n",
    "    fake_corpus = list()\n",
    "    real_corpus = list()\n",
    "    for eachDoc, eachCapDoc in zip(corpus, capital_corpus):\n",
    "        for _ in range(3):\n",
    "            eachFake, eachReal = gen_fake_once(eachDoc, eachCapDoc, w2m, c2w, num_bin, tgt_bin, num_rep)\n",
    "            fake_corpus.append(eachFake)\n",
    "        real_corpus.append(eachReal)\n",
    "\n",
    "    return fake_corpus, real_corpus\n",
    "\n",
    "def print_result(result_corpus):\n",
    "    \"\"\"\n",
    "    Auxiliary function that takes a result corpus (Dic or named tuple) and print them line-by-line\n",
    "    \"\"\"\n",
    "    for eachResult in result_corpus.keys():\n",
    "        print(f'Metric: {eachResult} -- {result_corpus[eachResult]}')\n",
    "    pass\n",
    "\n",
    "def write_result(result_corpus, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        for eachResult in result_corpus.keys():\n",
    "            f.write(f'Metric: {eachResult} -- {result_corpus[eachResult]}\\n')\n",
    "    pass\n",
    "\n",
    "def token2string(token_list):\n",
    "    \"\"\"\n",
    "    Auxiliary function that revert the \"work_tokenize\" -- Approximate imp.\n",
    "    \"\"\"\n",
    "    ans = list()\n",
    "    for eachToken in token_list:\n",
    "        if not isinstance(eachToken, str) or eachToken not in _PUNC_:\n",
    "            if len(ans) > 0 and ans[-1] in ['(', '\\n', '\\'']:\n",
    "                pass\n",
    "            else:\n",
    "                ans.append(\" \")\n",
    "            ans.append(str(eachToken))\n",
    "        else:\n",
    "            if str(eachToken) == \"(\":\n",
    "                ans.append(\" \")\n",
    "\n",
    "            ans.append(str(eachToken))\n",
    "\n",
    "    return \"\".join(ans[1:])\n",
    "\n",
    "def gen_fake_once(doc, cap_doc, w2m, c2w, num_bin, tgt_bin, num_rep):\n",
    "    assert len(doc) == len(cap_doc), \"DOC & CAP_DOC size inconsistent\"\n",
    "    \n",
    "    # Find concepts of given doc & Get TF-IDF\n",
    "    concept_candidates = defaultdict(float)\n",
    "\n",
    "    term_cnt = 0\n",
    "    for eachToken, tokenProp in pos_tag(doc):\n",
    "        if eachToken == '\\n':\n",
    "            continue\n",
    "\n",
    "        if tokenProp == 'NN':\n",
    "            concept_candidates[eachToken.lower()] += 1\n",
    "        if eachToken not in _PUNC_:\n",
    "            term_cnt += 1\n",
    "    \n",
    "    concept_list = list()\n",
    "    for eachConcept in concept_candidates.keys():\n",
    "        concept_candidates[eachConcept] /= float(term_cnt)\n",
    "        meta_tuple = w2m.get(eachConcept, None)\n",
    "        if meta_tuple is None:\n",
    "            continue\n",
    "\n",
    "        concept_candidates[eachConcept] *= meta_tuple[1]  # TF-IDF computation\n",
    "        concept_list.append((eachConcept, meta_tuple[0], concept_candidates[eachConcept]))\n",
    "\n",
    "    # Sort into bins\n",
    "    concept_list.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # Select & Scale replacement\n",
    "    if len(concept_list) < num_bin:\n",
    "        tgt_bin= floor((tgt_bin / num_bin) * len(concept_list))\n",
    "        num_bin = len(concept_list)\n",
    "        print(f'Number of cencepts is less than num_bin. Scaling num_bin to {num_bin}, target bin\\'s idx to {tgt_bin}')\n",
    "\n",
    "    num_concepts = len(concept_list)\n",
    "    split_factor = num_concepts // num_bin\n",
    "    bin_dict = defaultdict(list)\n",
    "    for idx in range(num_concepts):\n",
    "        bin_dict[idx // split_factor].append(concept_list[idx])\n",
    "\n",
    "    num_bin_concepts = len(bin_dict[tgt_bin])\n",
    "    if num_bin_concepts < num_rep or num_rep < 0:\n",
    "        print(f'Number of concepts ({num_bin_concepts}) in the target bin is less than num_rep. Scaling num_rep to {num_bin_concepts}')\n",
    "\n",
    "    replace_idxs = np.random.permutation(num_bin_concepts)[:min(num_bin_concepts, num_rep)]\n",
    "\n",
    "    # Build Replacement Mapping\n",
    "    replace_mapping = dict()\n",
    "    for eachIdx in replace_idxs:\n",
    "        c_token = bin_dict[tgt_bin][eachIdx]\n",
    "        token_nn = c2w[c_token[1]]\n",
    "        if len(token_nn) < 2:\n",
    "            replace_mapping[c_token[0]] = c_token[0]\n",
    "        else:\n",
    "            tmpIdx = np.random.choice(len(token_nn))\n",
    "            while token_nn[tmpIdx] == c_token[0]:\n",
    "                tmpIdx = np.random.choice(len(token_nn))\n",
    "            replace_mapping[c_token[0]] = token_nn[tmpIdx]\n",
    "\n",
    "    # Generate fake doc via replacement mapping\n",
    "    fake_doc = list()\n",
    "    real_doc = list()\n",
    "    change_cnt = 0\n",
    "    for idx in range(len(doc)):\n",
    "        current_token = replace_mapping.get(doc[idx], None)\n",
    "        if current_token is None:\n",
    "            real_doc.append(cap_doc[idx])\n",
    "            fake_doc.append(cap_doc[idx])\n",
    "        else:\n",
    "            real_doc.append(CGREEN + doc[idx] + CEND)\n",
    "            fake_doc.append(CRED + current_token + CEND)\n",
    "\n",
    "            if cap_doc[idx].istitle() or cap_doc[idx].isupper():\n",
    "                fake_doc.append(current_token.capitalize())\n",
    "            else:\n",
    "                fake_doc.append(current_token)\n",
    "    \n",
    "            change_cnt += 1\n",
    "\n",
    "    print(f\"Change Token Ratio of this doc is : {float(change_cnt) / len(real_doc)}\")\n",
    "\n",
    "    return fake_doc, real_doc\n",
    "\n",
    "def doc2corpus(data, corpus=None, with_tag=False, with_punc=False, lower=True):\n",
    "    \"\"\"\n",
    "    Auxiliary function that read document from a file and write them into a corpus (list_of_docs)\n",
    "    \"\"\"\n",
    "    for doc in data:\n",
    "        lines = doc.split('\\n')\n",
    "        new_doc = False\n",
    "        for eachLine in lines:\n",
    "            if eachLine == '\\n':\n",
    "                new_doc = False\n",
    "            else:\n",
    "                if with_tag:\n",
    "                    _, lineComp = line_process(eachLine, tag=True, remove_punc=operator.not_(with_punc), lower=lower)\n",
    "                else:\n",
    "                    lineComp, _ = line_process(eachLine, tag=False, remove_punc=operator.not_(with_punc), lower=lower)\n",
    "                if not new_doc:\n",
    "                    new_doc = True\n",
    "                    corpus.append(lineComp)\n",
    "                else:\n",
    "                    corpus[-1] += ['\\n']+ lineComp\n",
    "    return corpus\n",
    "\n",
    "    \n",
    "class DummyConfiguration(object):\n",
    "    def __init__(self, num_bin, tgt_bin, num_rep):\n",
    "        self.num_bin = num_bin\n",
    "        self.tgt_bin = tgt_bin\n",
    "        self.num_rep = num_rep\n",
    "        pass    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ba4a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T02:54:15.352383Z",
     "start_time": "2022-10-21T02:54:15.318923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading test data...')\n",
    "TEST_PATH = '../data/raw_data/cs_clean/test.txt'   \n",
    "\n",
    "Idx = [204, 645, 745, 791,  \n",
    "1120, 1226, 1263, 1578, 1633,  \n",
    "2203, 2369, 3005, 3357, 202,  \n",
    "3206, 2938]\n",
    "\n",
    "with open(TEST_PATH, 'r') as f:    \n",
    "    test_data = f.read().split('\\n\\n\\n')\n",
    "test_data = np.array(test_data)[Idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa1d224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T02:54:40.237504Z",
     "start_time": "2022-10-21T02:54:15.354407Z"
    },
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "META_PATH = 'outputs/cs_meta_neo.csv'\n",
    "\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "word2meta = dict()\n",
    "centroid2word = defaultdict(list)\n",
    "\n",
    "for instance_idx in range(len(meta_df.index)):\n",
    "    cinstance = meta_df.iloc[instance_idx]\n",
    "    word2meta[cinstance['Word']] = (cinstance['Centroids'], cinstance['IDF'])\n",
    "    centroid2word[cinstance['Centroids']].append(cinstance['Word'])\n",
    "    \n",
    "doc_corpus = doc2corpus(test_data, list(), with_punc=True)\n",
    "doc_corpus_capital = doc2corpus(test_data, list(), with_punc=True, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf9e814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T02:54:40.629872Z",
     "start_time": "2022-10-21T02:54:40.240271Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating fake docs...\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.08275862068965517\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.08275862068965517\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.08275862068965517\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.15172413793103448\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.15172413793103448\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.14482758620689656\n",
      "Number of concepts (9) in the target bin is less than num_rep. Scaling num_rep to 9\n",
      "Change Token Ratio of this doc is : 0.09210526315789473\n",
      "Number of concepts (9) in the target bin is less than num_rep. Scaling num_rep to 9\n",
      "Change Token Ratio of this doc is : 0.09210526315789473\n",
      "Number of concepts (9) in the target bin is less than num_rep. Scaling num_rep to 9\n",
      "Change Token Ratio of this doc is : 0.09210526315789473\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.22666666666666666\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.22666666666666666\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.21333333333333335\n",
      "Number of concepts (12) in the target bin is less than num_rep. Scaling num_rep to 12\n",
      "Change Token Ratio of this doc is : 0.10191082802547771\n",
      "Number of concepts (12) in the target bin is less than num_rep. Scaling num_rep to 12\n",
      "Change Token Ratio of this doc is : 0.11464968152866242\n",
      "Number of concepts (12) in the target bin is less than num_rep. Scaling num_rep to 12\n",
      "Change Token Ratio of this doc is : 0.11464968152866242\n",
      "Number of concepts (10) in the target bin is less than num_rep. Scaling num_rep to 10\n",
      "Change Token Ratio of this doc is : 0.13513513513513514\n",
      "Number of concepts (10) in the target bin is less than num_rep. Scaling num_rep to 10\n",
      "Change Token Ratio of this doc is : 0.13513513513513514\n",
      "Number of concepts (10) in the target bin is less than num_rep. Scaling num_rep to 10\n",
      "Change Token Ratio of this doc is : 0.11486486486486487\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.06896551724137931\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.07586206896551724\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.07586206896551724\n",
      "Number of concepts (11) in the target bin is less than num_rep. Scaling num_rep to 11\n",
      "Change Token Ratio of this doc is : 0.11538461538461539\n",
      "Number of concepts (11) in the target bin is less than num_rep. Scaling num_rep to 11\n",
      "Change Token Ratio of this doc is : 0.10897435897435898\n",
      "Number of concepts (11) in the target bin is less than num_rep. Scaling num_rep to 11\n",
      "Change Token Ratio of this doc is : 0.10897435897435898\n",
      "Number of concepts (9) in the target bin is less than num_rep. Scaling num_rep to 9\n",
      "Change Token Ratio of this doc is : 0.12056737588652482\n",
      "Number of concepts (9) in the target bin is less than num_rep. Scaling num_rep to 9\n",
      "Change Token Ratio of this doc is : 0.12056737588652482\n",
      "Number of concepts (9) in the target bin is less than num_rep. Scaling num_rep to 9\n",
      "Change Token Ratio of this doc is : 0.12056737588652482\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.14689265536723164\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.13559322033898305\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.13559322033898305\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.06622516556291391\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.07947019867549669\n",
      "Number of concepts (8) in the target bin is less than num_rep. Scaling num_rep to 8\n",
      "Change Token Ratio of this doc is : 0.0728476821192053\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.07926829268292683\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.07926829268292683\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.07926829268292683\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.13286713286713286\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.13286713286713286\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.13286713286713286\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.08333333333333333\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.08333333333333333\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.08333333333333333\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.09302325581395349\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.10852713178294573\n",
      "Number of concepts (7) in the target bin is less than num_rep. Scaling num_rep to 7\n",
      "Change Token Ratio of this doc is : 0.10077519379844961\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.10493827160493827\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.12345679012345678\n",
      "Number of concepts (13) in the target bin is less than num_rep. Scaling num_rep to 13\n",
      "Change Token Ratio of this doc is : 0.12962962962962962\n"
     ]
    }
   ],
   "source": [
    "num_bin = 2\n",
    "tgt_bin = 1\n",
    "\n",
    "conf = DummyConfiguration(num_bin, tgt_bin, -1)\n",
    "\n",
    "print('Generating fake docs...')\n",
    "fake_corpus, real_corpus = model_test(doc_corpus, doc_corpus_capital, word2meta, centroid2word, conf)\n",
    "\n",
    "original_docs = [token2string(item) for item in real_corpus]\n",
    "generate_docs = [token2string(item) for item in fake_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73cbd1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T02:54:40.653870Z",
     "start_time": "2022-10-21T02:54:40.631330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Normalized \u001b[46minformation\u001b[0m Distance \n",
      "The normalized \u001b[46minformation\u001b[0m distance is a universal distance measure for objects of all kinds. It is based on \u001b[46mkolmogorov\u001b[0m \u001b[46mcomplexity\u001b[0m and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the \u001b[46mkolmogorov\u001b[0m \u001b[46mcomplexity\u001b[0m if the objects have a \u001b[46mstring\u001b[0m representation. Second, for names and abstract concepts, page \u001b[46mcount\u001b[0m statistics from the World Wide Web can be used. These practical realizations of the normalized \u001b[46minformation\u001b[0m distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This \u001b[46mchapter\u001b[0m discusses the theoretical foundations of the normalized \u001b[46minformation\u001b[0m distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to \u001b[46mmusic\u001b[0m clustering to machine translation.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Normalized \u001b[41mwikipedia\u001b[0m Wikipedia Distance \n",
      "The normalized \u001b[41mwikipedia\u001b[0m wikipedia distance is a universal distance measure for objects of all kinds. It is based on \u001b[41muniversal\u001b[0m Universal \u001b[41mexpense\u001b[0m expense and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the \u001b[41muniversal\u001b[0m Universal \u001b[41mexpense\u001b[0m expense if the objects have a \u001b[41mrelaxed\u001b[0m relaxed representation. Second, for names and abstract concepts, \u001b[41mengagement\u001b[0m engagement \u001b[41mscaling\u001b[0m scaling statistics from the World Wide Web can be used. These practical realizations of the normalized \u001b[41mwikipedia\u001b[0m wikipedia distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This \u001b[41mletter\u001b[0m letter discusses the theoretical foundations of the normalized \u001b[41mwikipedia\u001b[0m wikipedia distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to music clustering to machine translation.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Normalized \u001b[41mreviews\u001b[0m Reviews Distance \n",
      "The normalized \u001b[41mreviews\u001b[0m reviews distance is a universal distance measure for objects of all kinds. It is based on \u001b[41mstatistical\u001b[0m Statistical \u001b[41mburden\u001b[0m burden and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the \u001b[41mstatistical\u001b[0m Statistical \u001b[41mburden\u001b[0m burden if the objects have a \u001b[41mstrings\u001b[0m strings representation. Second, for names and abstract concepts, page \u001b[41mlow\u001b[0m low statistics from the World Wide Web can be used. These practical realizations of the normalized \u001b[41mreviews\u001b[0m reviews distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This \u001b[41mmanuscript\u001b[0m manuscript discusses the theoretical foundations of the normalized \u001b[41mreviews\u001b[0m reviews distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to \u001b[41mtopic\u001b[0m topic clustering to machine translation.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Normalized \u001b[41mengagement\u001b[0m Engagement Distance \n",
      "The normalized \u001b[41mengagement\u001b[0m engagement distance is a universal distance measure for objects of all kinds. It is based on \u001b[41msystematic\u001b[0m Systematic \u001b[41moverhead\u001b[0m overhead and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the \u001b[41msystematic\u001b[0m Systematic \u001b[41moverhead\u001b[0m overhead if the objects have a \u001b[41mhomomorphisms\u001b[0m homomorphisms representation. Second, for names and abstract concepts, page \u001b[41mdistribution\u001b[0m distribution statistics from the World Wide Web can be used. These practical realizations of the normalized \u001b[41mengagement\u001b[0m engagement distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This \u001b[41mmanuscript\u001b[0m manuscript discusses the theoretical foundations of the normalized \u001b[41mengagement\u001b[0m engagement distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to \u001b[41mlanguage\u001b[0m language clustering to machine translation.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Variational Dual-Tree Framework for Large-Scale \u001b[46mtransition\u001b[0m \u001b[46mmatrix\u001b[0m \u001b[46mapproximation\u001b[0m \n",
      "In recent years, non-parametric methods utilizing \u001b[46mrandom\u001b[0m walks on \u001b[46mgraphs\u001b[0m have been used to solve a wide range of machine learning problems, but in their simplest form they do not scale well due to the quadratic complexity. In this paper, a new dual-tree based variational approach for approximating the \u001b[46mtransition\u001b[0m \u001b[46mmatrix\u001b[0m and efficiently performing the \u001b[46mrandom\u001b[0m \u001b[46mwalk\u001b[0m is proposed. The approach exploits a \u001b[46mconnection\u001b[0m between kernel \u001b[46mdensity\u001b[0m estimation, \u001b[46mmixture\u001b[0m \u001b[46mmodeling\u001b[0m, and \u001b[46mrandom\u001b[0m \u001b[46mwalk\u001b[0m on \u001b[46mgraphs\u001b[0m in an optimization of the \u001b[46mtransition\u001b[0m \u001b[46mmatrix\u001b[0m for the data graph that ties together edge transitions probabilities that are similar. Compared to the de facto standard \u001b[46mapproximation\u001b[0m method based on k-nearestneighbors, we demonstrate order of magnitudes speedup without sacrificing accuracy for Label \u001b[46mpropagation\u001b[0m tasks on \u001b[46mbenchmark\u001b[0m data sets in semi-supervised learning.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Variational Dual-Tree Framework for Large-Scale \u001b[41mmixing\u001b[0m Mixing \u001b[41mcompressed\u001b[0m Compressed \u001b[41mconstraint\u001b[0m Constraint \n",
      "In recent years, non-parametric methods utilizing \u001b[41mcapacity\u001b[0m capacity walks on \u001b[41mmultiplex\u001b[0m multiplex have been used to solve a wide range of machine \u001b[41mhashing\u001b[0m hashing problems, but in their simplest form they do not scale well due to the quadratic complexity. In this paper, a new dual-tree based variational approach for approximating the \u001b[41mmixing\u001b[0m mixing \u001b[41mcompressed\u001b[0m compressed and efficiently performing the \u001b[41mcapacity\u001b[0m capacity \u001b[41mproximity\u001b[0m proximity is proposed. The approach exploits a \u001b[41mcausation\u001b[0m causation between kernel density estimation, \u001b[41mlowdimensional\u001b[0m lowdimensional \u001b[41minteractivity\u001b[0m interactivity, and \u001b[41mcapacity\u001b[0m capacity \u001b[41mproximity\u001b[0m proximity on \u001b[41mmultiplex\u001b[0m multiplex in an optimization of the \u001b[41mmixing\u001b[0m mixing \u001b[41mcompressed\u001b[0m compressed for the data graph that ties together edge transitions probabilities that are similar. Compared to the de facto standard \u001b[41mconstraint\u001b[0m constraint method based on k-nearestneighbors, we demonstrate order of magnitudes speedup without sacrificing accuracy for Label \u001b[41mconditioning\u001b[0m Conditioning tasks on \u001b[41mhmdb51\u001b[0m hmdb51 data sets in semi-supervised \u001b[41mhashing\u001b[0m hashing.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Variational Dual-Tree Framework for Large-Scale \u001b[41mdg\u001b[0m Dg \u001b[41mseries\u001b[0m Series \u001b[41msemidefinite\u001b[0m Semidefinite \n",
      "In recent years, non-parametric methods utilizing \u001b[41miid\u001b[0m iid walks on \u001b[41massortative\u001b[0m assortative have been used to solve a wide range of machine \u001b[41mmil\u001b[0m mil problems, but in their simplest form they do not scale well due to the quadratic complexity. In this paper, a new dual-tree based variational approach for approximating the \u001b[41mdg\u001b[0m dg \u001b[41mseries\u001b[0m series and efficiently performing the \u001b[41miid\u001b[0m iid \u001b[41mmotifs\u001b[0m motifs is proposed. The approach exploits a \u001b[41mpresence\u001b[0m presence between kernel \u001b[41mlevelset\u001b[0m levelset estimation, \u001b[41msubspace\u001b[0m subspace modeling, and \u001b[41miid\u001b[0m iid \u001b[41mmotifs\u001b[0m motifs on \u001b[41massortative\u001b[0m assortative in an optimization of the \u001b[41mdg\u001b[0m dg \u001b[41mseries\u001b[0m series for the data graph that ties together edge transitions probabilities that are similar. Compared to the de facto standard \u001b[41msemidefinite\u001b[0m semidefinite method based on k-nearestneighbors, we demonstrate order of magnitudes speedup without sacrificing accuracy for Label \u001b[41mdownsampling\u001b[0m Downsampling tasks on \u001b[41mmit\u001b[0m mit data sets in semi-supervised \u001b[41mmil\u001b[0m mil.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Variational Dual-Tree Framework for Large-Scale \u001b[41mmixing\u001b[0m Mixing \u001b[41mprincipal\u001b[0m Principal \u001b[41mvariance\u001b[0m Variance \n",
      "In recent years, non-parametric methods utilizing \u001b[41municast\u001b[0m unicast walks on \u001b[41mmetabolic\u001b[0m metabolic have been used to solve a wide range of machine learning problems, but in their simplest form they do not scale well due to the quadratic complexity. In this paper, a new dual-tree based variational approach for approximating the \u001b[41mmixing\u001b[0m mixing \u001b[41mprincipal\u001b[0m principal and efficiently performing the \u001b[41municast\u001b[0m unicast \u001b[41mconductance\u001b[0m conductance is proposed. The approach exploits a \u001b[41minefficiency\u001b[0m inefficiency between kernel \u001b[41mtelescoping\u001b[0m telescoping estimation, \u001b[41maffinity\u001b[0m affinity \u001b[41mcrowdsourcing\u001b[0m crowdsourcing, and \u001b[41municast\u001b[0m unicast \u001b[41mconductance\u001b[0m conductance on \u001b[41mmetabolic\u001b[0m metabolic in an optimization of the \u001b[41mmixing\u001b[0m mixing \u001b[41mprincipal\u001b[0m principal for the data graph that ties together edge transitions probabilities that are similar. Compared to the de facto standard \u001b[41mvariance\u001b[0m variance method based on k-nearestneighbors, we demonstrate order of magnitudes speedup without sacrificing accuracy for Label \u001b[41mfitting\u001b[0m Fitting tasks on \u001b[41mcomparison\u001b[0m comparison data sets in semi-supervised learning.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Personalized News \u001b[46mrecommendation\u001b[0m with \u001b[46mcontext\u001b[0m Trees \n",
      "The \u001b[46mprofusion\u001b[0m of online news articles makes it difficult to find interesting articles, a problem that can be assuaged by using a \u001b[46mrecommender\u001b[0m system to bring the most relevant news stories to readers. However, news \u001b[46mrecommendation\u001b[0m is challenging because the most relevant articles are often new \u001b[46mcontent\u001b[0m seen by few users. In addition, they are subject to trends and \u001b[46mpreference\u001b[0m changes over time, and in many cases we do not have sufficient information to profile the \u001b[46mreader\u001b[0m. \n",
      "In this paper, we introduce a class of news \u001b[46mrecommendation\u001b[0m systems based on \u001b[46mcontext\u001b[0m trees. They can provide high-quality news \u001b[46mrecommendation\u001b[0m to anonymous visitors based on present browsing \u001b[46mbehaviour\u001b[0m. We show that context-tree \u001b[46mrecommender\u001b[0m systems provide good prediction accuracy and \u001b[46mrecommendation\u001b[0m novelty, and they are sufficiently flexible to capture the unique properties of news articles.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Personalized News \u001b[41mwatch\u001b[0m Watch with \u001b[41mentailment\u001b[0m Entailment Trees \n",
      "The \u001b[41mstateelimination\u001b[0m stateelimination of online news articles makes it difficult to find interesting articles, a problem that can be assuaged by using a \u001b[41mnetflix\u001b[0m netflix system to bring the most relevant news stories to readers. However, news \u001b[41mwatch\u001b[0m watch is challenging because the most relevant articles are often new \u001b[41mprofile\u001b[0m profile seen by few users. In addition, they are subject to trends and \u001b[41mdrifts\u001b[0m drifts changes over time, and in many cases we do not have sufficient information to profile the reader. \n",
      "In this paper, we introduce a class of news \u001b[41mwatch\u001b[0m watch systems based on \u001b[41mentailment\u001b[0m entailment trees. They can provide high-quality news \u001b[41mwatch\u001b[0m watch to anonymous visitors based on present browsing \u001b[41mpreference\u001b[0m preference. We show that context-tree \u001b[41mnetflix\u001b[0m netflix systems provide good prediction accuracy and \u001b[41mwatch\u001b[0m watch \u001b[41manalysis\u001b[0m analysis, and they are sufficiently flexible to capture the unique properties of news articles.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Personalized News \u001b[41mwikipedia\u001b[0m Wikipedia with \u001b[41mdocument\u001b[0m Document Trees \n",
      "The \u001b[41mmicrocosmos\u001b[0m microcosmos of online news articles makes it difficult to find interesting articles, a problem that can be assuaged by using a \u001b[41mgeotagged\u001b[0m geotagged system to bring the most relevant news stories to readers. However, news \u001b[41mwikipedia\u001b[0m wikipedia is challenging because the most relevant articles are often new \u001b[41mposting\u001b[0m posting seen by few users. In addition, they are subject to trends and preference changes over time, and in many cases we do not have sufficient information to profile the \u001b[41mattempts\u001b[0m attempts. \n",
      "In this paper, we introduce a class of news \u001b[41mwikipedia\u001b[0m wikipedia systems based on \u001b[41mdocument\u001b[0m document trees. They can provide high-quality news \u001b[41mwikipedia\u001b[0m wikipedia to anonymous visitors based on present browsing \u001b[41mgaps\u001b[0m gaps. We show that context-tree \u001b[41mgeotagged\u001b[0m geotagged systems provide good prediction accuracy and \u001b[41mwikipedia\u001b[0m wikipedia \u001b[41mmethodological\u001b[0m methodological, and they are sufficiently flexible to capture the unique properties of news articles.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Personalized News \u001b[41mvenue\u001b[0m Venue with \u001b[41mpassage\u001b[0m Passage Trees \n",
      "The \u001b[41mauviitk\u001b[0m auviitk of online news articles makes it difficult to find interesting articles, a problem that can be assuaged by using a \u001b[41mauthor\u001b[0m author system to bring the most relevant news stories to readers. However, news \u001b[41mvenue\u001b[0m venue is challenging because the most relevant articles are often new \u001b[41minformation\u001b[0m information seen by few users. In addition, they are subject to trends and \u001b[41mdeficiency\u001b[0m deficiency changes over time, and in many cases we do not have sufficient information to profile the \u001b[41mfoundation\u001b[0m foundation. \n",
      "In this paper, we introduce a class of news \u001b[41mvenue\u001b[0m venue systems based on \u001b[41mpassage\u001b[0m passage trees. They can provide high-quality news \u001b[41mvenue\u001b[0m venue to anonymous visitors based on present browsing \u001b[41minstability\u001b[0m instability. We show that context-tree \u001b[41mauthor\u001b[0m author systems provide good prediction accuracy and \u001b[41mvenue\u001b[0m venue novelty, and they are sufficiently flexible to capture the unique properties of news articles.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Binary \u001b[46mtree\u001b[0m based Chinese Word \u001b[46msegmentation\u001b[0m \n",
      "Chinese word \u001b[46msegmentation\u001b[0m is a fundamental task for Chinese language processing. The \u001b[46mgranularity\u001b[0m \u001b[46mmismatch\u001b[0m \u001b[46mproblem\u001b[0m is the main \u001b[46mcause\u001b[0m of the errors. This paper showed that the binary \u001b[46mtree\u001b[0m representation can store outputs with different \u001b[46mgranularity\u001b[0m. A binary \u001b[46mtree\u001b[0m based \u001b[46mframework\u001b[0m is also designed to overcome the \u001b[46mgranularity\u001b[0m \u001b[46mmismatch\u001b[0m \u001b[46mproblem\u001b[0m. There are two steps in this \u001b[46mframework\u001b[0m, namely \u001b[46mtree\u001b[0m \u001b[46mbuilding\u001b[0m and \u001b[46mtree\u001b[0m \u001b[46mpruning\u001b[0m. The \u001b[46mtree\u001b[0m \u001b[46mpruning\u001b[0m step is specially designed to focus on the \u001b[46mgranularity\u001b[0m \u001b[46mproblem\u001b[0m. Previous work for Chinese word \u001b[46msegmentation\u001b[0m such as the sequence \u001b[46mtagging\u001b[0m can be easily employed in this \u001b[46mframework\u001b[0m. This \u001b[46mframework\u001b[0m can also provide quantitative \u001b[46merror\u001b[0m analysis methods. The experiments showed that after using a more sophisticated \u001b[46mtree\u001b[0m \u001b[46mpruning\u001b[0m function for a state-of-the-art conditional random field based \u001b[46mbaseline\u001b[0m, the \u001b[46merror\u001b[0m reduction can be up to 20%.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Binary \u001b[41mtiling\u001b[0m Tiling based Chinese \u001b[41mbert\u001b[0m Bert \u001b[41mimbalanced\u001b[0m Imbalanced \n",
      "Chinese \u001b[41mbert\u001b[0m bert \u001b[41mimbalanced\u001b[0m imbalanced is a fundamental task for Chinese language processing. The \u001b[41mremaining\u001b[0m remaining \u001b[41minterclass\u001b[0m interclass \u001b[41mpolicy\u001b[0m policy is the main \u001b[41mexhibit\u001b[0m exhibit of the errors. This paper showed that the binary \u001b[41mtiling\u001b[0m tiling representation can store outputs with different \u001b[41mremaining\u001b[0m remaining. A binary \u001b[41mtiling\u001b[0m tiling based \u001b[41mmethod\u001b[0m method is also designed to overcome the \u001b[41mremaining\u001b[0m remaining \u001b[41minterclass\u001b[0m interclass \u001b[41mpolicy\u001b[0m policy. There are two steps in this \u001b[41mmethod\u001b[0m method, namely \u001b[41mtiling\u001b[0m tiling \u001b[41mversioning\u001b[0m versioning and \u001b[41mtiling\u001b[0m tiling \u001b[41msr\u001b[0m sr. The \u001b[41mtiling\u001b[0m tiling \u001b[41msr\u001b[0m sr step is specially designed to focus on the \u001b[41mremaining\u001b[0m remaining \u001b[41mpolicy\u001b[0m policy. Previous work for Chinese \u001b[41mbert\u001b[0m bert \u001b[41mimbalanced\u001b[0m imbalanced such as the sequence tagging can be easily employed in this \u001b[41mmethod\u001b[0m method. This \u001b[41mmethod\u001b[0m method can also provide quantitative \u001b[41mpercent\u001b[0m percent analysis methods. The experiments showed that after using a more sophisticated \u001b[41mtiling\u001b[0m tiling \u001b[41msr\u001b[0m sr function for a state-of-the-art conditional random field based \u001b[41malexnet\u001b[0m alexnet, the \u001b[41mpercent\u001b[0m percent reduction can be up to 20%.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Binary \u001b[41mcutting\u001b[0m Cutting based Chinese \u001b[41mpos\u001b[0m Pos \u001b[41mreidentification\u001b[0m Reidentification \n",
      "Chinese \u001b[41mpos\u001b[0m pos \u001b[41mreidentification\u001b[0m reidentification is a fundamental task for Chinese language processing. The \u001b[41mlogin\u001b[0m login \u001b[41msymmetry\u001b[0m symmetry \u001b[41moffpolicy\u001b[0m offpolicy is the main \u001b[41myield\u001b[0m yield of the errors. This paper showed that the binary \u001b[41mcutting\u001b[0m cutting representation can store outputs with different \u001b[41mlogin\u001b[0m login. A binary \u001b[41mcutting\u001b[0m cutting based \u001b[41midea\u001b[0m idea is also designed to overcome the \u001b[41mlogin\u001b[0m login \u001b[41msymmetry\u001b[0m symmetry \u001b[41moffpolicy\u001b[0m offpolicy. There are two steps in this \u001b[41midea\u001b[0m idea, namely \u001b[41mcutting\u001b[0m cutting \u001b[41mcrowdsourcing\u001b[0m crowdsourcing and \u001b[41mcutting\u001b[0m cutting \u001b[41msimplification\u001b[0m simplification. The \u001b[41mcutting\u001b[0m cutting \u001b[41msimplification\u001b[0m simplification step is specially designed to focus on the \u001b[41mlogin\u001b[0m login \u001b[41moffpolicy\u001b[0m offpolicy. Previous work for Chinese \u001b[41mpos\u001b[0m pos \u001b[41mreidentification\u001b[0m reidentification such as the sequence tagging can be easily employed in this \u001b[41midea\u001b[0m idea. This \u001b[41midea\u001b[0m idea can also provide quantitative \u001b[41meer\u001b[0m eer analysis methods. The experiments showed that after using a more sophisticated \u001b[41mcutting\u001b[0m cutting \u001b[41msimplification\u001b[0m simplification function for a state-of-the-art conditional random field based \u001b[41mnist\u001b[0m nist, the \u001b[41meer\u001b[0m eer reduction can be up to 20%.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Binary \u001b[41mhypergraph\u001b[0m Hypergraph based Chinese Word \u001b[41mda\u001b[0m Da \n",
      "Chinese word \u001b[41mda\u001b[0m da is a fundamental task for Chinese language processing. The \u001b[41mrecomputation\u001b[0m recomputation \u001b[41minterdisciplinarity\u001b[0m interdisciplinarity \u001b[41mimitation\u001b[0m imitation is the main \u001b[41madmit\u001b[0m admit of the errors. This paper showed that the binary \u001b[41mhypergraph\u001b[0m hypergraph representation can store outputs with different \u001b[41mrecomputation\u001b[0m recomputation. A binary \u001b[41mhypergraph\u001b[0m hypergraph based \u001b[41mversion\u001b[0m version is also designed to overcome the \u001b[41mrecomputation\u001b[0m recomputation \u001b[41minterdisciplinarity\u001b[0m interdisciplinarity \u001b[41mimitation\u001b[0m imitation. There are two steps in this \u001b[41mversion\u001b[0m version, namely \u001b[41mhypergraph\u001b[0m hypergraph \u001b[41minstallation\u001b[0m installation and \u001b[41mhypergraph\u001b[0m hypergraph \u001b[41mrefinement\u001b[0m refinement. The \u001b[41mhypergraph\u001b[0m hypergraph \u001b[41mrefinement\u001b[0m refinement step is specially designed to focus on the \u001b[41mrecomputation\u001b[0m recomputation \u001b[41mimitation\u001b[0m imitation. Previous work for Chinese word \u001b[41mda\u001b[0m da such as the sequence \u001b[41mtranslation\u001b[0m translation can be easily employed in this \u001b[41mversion\u001b[0m version. This \u001b[41mversion\u001b[0m version can also provide quantitative \u001b[41mmargin\u001b[0m margin analysis methods. The experiments showed that after using a more sophisticated \u001b[41mhypergraph\u001b[0m hypergraph \u001b[41mrefinement\u001b[0m refinement function for a state-of-the-art conditional random field based \u001b[41mlfw\u001b[0m lfw, the \u001b[41mmargin\u001b[0m margin reduction can be up to 20%.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Approximate \u001b[46moptimality\u001b[0m with bounded \u001b[46mregret\u001b[0m in dynamic \u001b[46mmatching\u001b[0m models \n",
      "We consider a discrete-time \u001b[46mbipartite\u001b[0m \u001b[46mmatching\u001b[0m model with random arrivals of units of \u001b[46msupply\u001b[0m and demand that can wait in queues located at the nodes in the network. A \u001b[46mcontrol\u001b[0m policy determines which are matched at each time. The focus is on the infinite-horizon average-cost optimal \u001b[46mcontrol\u001b[0m problem. A \u001b[46mrelaxation\u001b[0m of the stochastic \u001b[46mcontrol\u001b[0m problem is proposed, which is found to be a special case of an \u001b[46minventory\u001b[0m model, as treated in the classical theory of Clark and \u001b[46mscarf\u001b[0m. The optimal policy for the \u001b[46mrelaxation\u001b[0m admits a closed-form \u001b[46mexpression\u001b[0m. Based on the policy for this \u001b[46mrelaxation\u001b[0m, a new \u001b[46mmatching\u001b[0m policy is proposed. For a parameterized family of models in which the network \u001b[46mload\u001b[0m approaches capacity, this policy is shown to be approximately optimal, with bounded \u001b[46mregret\u001b[0m, even though the average cost grows without bound.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Approximate \u001b[41mgame\u001b[0m game with bounded \u001b[41mvalue\u001b[0m value in dynamic \u001b[41minterpolation\u001b[0m interpolation models \n",
      "We consider a discrete-time \u001b[41mcascading\u001b[0m cascading \u001b[41minterpolation\u001b[0m interpolation model with random arrivals of units of \u001b[41madvertiser\u001b[0m advertiser and demand that can wait in queues located at the nodes in the network. A control policy determines which are matched at each time. The focus is on the infinite-horizon average-cost optimal control problem. A \u001b[41mjacobian\u001b[0m jacobian of the stochastic control problem is proposed, which is found to be a special case of an \u001b[41mdesire\u001b[0m desire model, as treated in the classical theory of \u001b[41mhelbing\u001b[0m Helbing and \u001b[41msmk\u001b[0m Smk. The optimal policy for the \u001b[41mjacobian\u001b[0m jacobian admits a closed-form \u001b[41mformulae\u001b[0m formulae. Based on the policy for this \u001b[41mjacobian\u001b[0m jacobian, a new \u001b[41minterpolation\u001b[0m interpolation policy is proposed. For a parameterized family of models in which the network \u001b[41mrouting\u001b[0m routing approaches capacity, this policy is shown to be approximately optimal, with bounded \u001b[41mvalue\u001b[0m value, even though the average cost grows without bound.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Approximate \u001b[41moffpolicy\u001b[0m offpolicy with bounded \u001b[41mutility\u001b[0m utility in dynamic \u001b[41mrejection\u001b[0m rejection models \n",
      "We consider a discrete-time \u001b[41minterdependent\u001b[0m interdependent \u001b[41mrejection\u001b[0m rejection model with random arrivals of units of supply and demand that can wait in queues located at the nodes in the network. A \u001b[41mrobots\u001b[0m robots policy determines which are matched at each time. The focus is on the infinite-horizon average-cost optimal \u001b[41mrobots\u001b[0m robots problem. A \u001b[41mmultiplicative\u001b[0m multiplicative of the stochastic \u001b[41mrobots\u001b[0m robots problem is proposed, which is found to be a special case of an \u001b[41mchurn\u001b[0m churn model, as treated in the classical theory of \u001b[41maziz\u001b[0m Aziz and \u001b[41mtsujii\u001b[0m Tsujii. The optimal policy for the \u001b[41mmultiplicative\u001b[0m multiplicative admits a closed-form \u001b[41mformulas\u001b[0m formulas. Based on the policy for this \u001b[41mmultiplicative\u001b[0m multiplicative, a new \u001b[41mrejection\u001b[0m rejection policy is proposed. For a parameterized family of models in which the network \u001b[41mbackhaul\u001b[0m backhaul approaches capacity, this policy is shown to be approximately optimal, with bounded \u001b[41mutility\u001b[0m utility, even though the average cost grows without bound.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Approximate \u001b[41mmyopic\u001b[0m myopic with bounded \u001b[41mprofit\u001b[0m profit in dynamic \u001b[41mshrinkage\u001b[0m shrinkage models \n",
      "We consider a discrete-time \u001b[41mtopological\u001b[0m topological \u001b[41mshrinkage\u001b[0m shrinkage model with random arrivals of units of \u001b[41mrecruitment\u001b[0m recruitment and demand that can wait in queues located at the nodes in the network. A \u001b[41mmoving\u001b[0m moving policy determines which are matched at each time. The focus is on the infinite-horizon average-cost optimal \u001b[41mmoving\u001b[0m moving problem. A \u001b[41mquadratic\u001b[0m quadratic of the stochastic \u001b[41mmoving\u001b[0m moving problem is proposed, which is found to be a special case of an \u001b[41madvertiser\u001b[0m advertiser model, as treated in the classical theory of Clark and \u001b[41mwriteability\u001b[0m Writeability. The optimal policy for the \u001b[41mquadratic\u001b[0m quadratic admits a closed-form \u001b[41mformulas\u001b[0m formulas. Based on the policy for this \u001b[41mquadratic\u001b[0m quadratic, a new \u001b[41mshrinkage\u001b[0m shrinkage policy is proposed. For a parameterized family of models in which the network \u001b[41mbattery\u001b[0m battery approaches capacity, this policy is shown to be approximately optimal, with bounded \u001b[41mprofit\u001b[0m profit, even though the average cost grows without bound.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Tomographic \u001b[46mimage\u001b[0m \u001b[46mreconstruction\u001b[0m using \u001b[46mtraining\u001b[0m images \n",
      "We describe and examine an algorithm for tomographic \u001b[46mimage\u001b[0m \u001b[46mreconstruction\u001b[0m where prior knowledge about the \u001b[46msolution\u001b[0m is available in the form of \u001b[46mtraining\u001b[0m images. We first construct a nonnegative dictionary based on prototype elements from the \u001b[46mtraining\u001b[0m images; this problem is formulated as a regularized non-negative matrix \u001b[46mfactorization\u001b[0m. Incorporating the dictionary as a prior in a convex \u001b[46mreconstruction\u001b[0m problem, we then find an approximate \u001b[46msolution\u001b[0m with a sparse representation in the dictionary. The dictionary is applied to non-overlapping patches of the \u001b[46mimage\u001b[0m, which reduces the computational complexity compared to other algorithms. Computational experiments clarify the choice and \u001b[46minterplay\u001b[0m of the model parameters and the \u001b[46mregularization\u001b[0m parameters, and we show that in few-projection low-dose settings our algorithm is competitive with total \u001b[46mvariation\u001b[0m \u001b[46mregularization\u001b[0m and tends to include more \u001b[46mtexture\u001b[0m and more correct edges.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Tomographic \u001b[41mmask\u001b[0m Mask \u001b[41minversion\u001b[0m Inversion using \u001b[41mreranking\u001b[0m Reranking images \n",
      "We describe and examine an algorithm for tomographic \u001b[41mmask\u001b[0m mask \u001b[41minversion\u001b[0m inversion where prior knowledge about the \u001b[41msystem\u001b[0m system is available in the form of \u001b[41mreranking\u001b[0m reranking images. We first construct a nonnegative \u001b[41mpairwise\u001b[0m pairwise based on prototype elements from the \u001b[41mreranking\u001b[0m reranking images; this problem is formulated as a regularized non-negative matrix \u001b[41mtruncation\u001b[0m truncation. Incorporating the \u001b[41mpairwise\u001b[0m pairwise as a prior in a convex \u001b[41minversion\u001b[0m inversion problem, we then find an approximate \u001b[41msystem\u001b[0m system with a sparse representation in the \u001b[41mpairwise\u001b[0m pairwise. The \u001b[41mpairwise\u001b[0m pairwise is applied to non-overlapping patches of the \u001b[41mmask\u001b[0m mask, which reduces the computational complexity compared to other algorithms. Computational experiments clarify the choice and \u001b[41mgaps\u001b[0m gaps of the model parameters and the \u001b[41mloglikelihood\u001b[0m loglikelihood parameters, and we show that in few-projection low-dose settings our algorithm is competitive with total \u001b[41mcertainty\u001b[0m certainty \u001b[41mloglikelihood\u001b[0m loglikelihood and tends to include more texture and more correct edges.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Tomographic \u001b[41mmask\u001b[0m Mask \u001b[41mcorrection\u001b[0m Correction using \u001b[41mtransfer\u001b[0m Transfer images \n",
      "We describe and examine an algorithm for tomographic \u001b[41mmask\u001b[0m mask \u001b[41mcorrection\u001b[0m correction where prior knowledge about the \u001b[41mmechanism\u001b[0m mechanism is available in the form of \u001b[41mtransfer\u001b[0m transfer images. We first construct a nonnegative \u001b[41mradon\u001b[0m radon based on prototype elements from the \u001b[41mtransfer\u001b[0m transfer images; this problem is formulated as a regularized non-negative matrix \u001b[41mdehazing\u001b[0m dehazing. Incorporating the \u001b[41mradon\u001b[0m radon as a prior in a convex \u001b[41mcorrection\u001b[0m correction problem, we then find an approximate \u001b[41mmechanism\u001b[0m mechanism with a sparse representation in the \u001b[41mradon\u001b[0m radon. The \u001b[41mradon\u001b[0m radon is applied to non-overlapping patches of the \u001b[41mmask\u001b[0m mask, which reduces the computational complexity compared to other algorithms. Computational experiments clarify the choice and \u001b[41muniformity\u001b[0m uniformity of the model parameters and the \u001b[41mentropy\u001b[0m entropy parameters, and we show that in few-projection low-dose settings our algorithm is competitive with total variation \u001b[41mentropy\u001b[0m entropy and tends to include more \u001b[41mheatmap\u001b[0m heatmap and more correct edges.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Tomographic \u001b[41mpixellevel\u001b[0m Pixellevel \u001b[41mregistration\u001b[0m Registration using \u001b[41mcostsensitive\u001b[0m Costsensitive images \n",
      "We describe and examine an algorithm for tomographic \u001b[41mpixellevel\u001b[0m pixellevel \u001b[41mregistration\u001b[0m registration where prior knowledge about the \u001b[41mprocedure\u001b[0m procedure is available in the form of \u001b[41mcostsensitive\u001b[0m costsensitive images. We first construct a nonnegative dictionary based on prototype elements from the \u001b[41mcostsensitive\u001b[0m costsensitive images; this problem is formulated as a regularized non-negative matrix \u001b[41mblending\u001b[0m blending. Incorporating the dictionary as a prior in a convex \u001b[41mregistration\u001b[0m registration problem, we then find an approximate \u001b[41mprocedure\u001b[0m procedure with a sparse representation in the dictionary. The dictionary is applied to non-overlapping patches of the \u001b[41mpixellevel\u001b[0m pixellevel, which reduces the computational complexity compared to other algorithms. Computational experiments clarify the choice and \u001b[41mtendency\u001b[0m tendency of the model parameters and the \u001b[41mminimization\u001b[0m minimization parameters, and we show that in few-projection low-dose settings our algorithm is competitive with total \u001b[41minterdependency\u001b[0m interdependency \u001b[41mminimization\u001b[0m minimization and tends to include more \u001b[41mresolution\u001b[0m resolution and more correct edges.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Reasoning about Unmodelled Concepts- Incorporating Class Taxonomies in Probabilistic Relational Models \n",
      "A \u001b[46mkey\u001b[0m problem in the application of first-order probabilistic methods is the enormous \u001b[46msize\u001b[0m of graphical models they imply. The \u001b[46msize\u001b[0m results from the possible worlds that can be generated by a domain of objects and relations. One of the reasons for this explosion is that so far the approaches do not sufficiently exploit the \u001b[46mstructure\u001b[0m and \u001b[46msimilarity\u001b[0m of possible worlds in order to encode the models more compactly. We propose fuzzy \u001b[46minference\u001b[0m in Markov logic networks, which enables the use of taxonomic \u001b[46mknowledge\u001b[0m as a source of imposing \u001b[46mstructure\u001b[0m onto possible worlds. We show that by exploiting this \u001b[46mstructure\u001b[0m, probability distributions can be represented more compactly and that the reasoning systems become capable of reasoning about concepts not contained in the probabilistic \u001b[46mknowledge\u001b[0m \u001b[46mbase\u001b[0m.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Reasoning about Unmodelled Concepts- Incorporating Class Taxonomies in Probabilistic Relational Models \n",
      "A \u001b[41mcapable\u001b[0m capable problem in the application of first-order probabilistic methods is the enormous \u001b[41mprescribed\u001b[0m prescribed of graphical models they imply. The \u001b[41mprescribed\u001b[0m prescribed results from the possible worlds that can be generated by a domain of objects and relations. One of the reasons for this \u001b[41mvariety\u001b[0m variety is that so far the approaches do not sufficiently exploit the \u001b[41moutdegree\u001b[0m outdegree and \u001b[41mbilinear\u001b[0m bilinear of possible worlds in order to encode the models more compactly. We propose fuzzy \u001b[41mremoval\u001b[0m removal in Markov logic networks, which enables the use of taxonomic knowledge as a source of imposing \u001b[41moutdegree\u001b[0m outdegree onto possible worlds. We show that by exploiting this \u001b[41moutdegree\u001b[0m outdegree, probability distributions can be represented more compactly and that the reasoning systems become capable of reasoning about concepts not contained in the probabilistic knowledge \u001b[41mconsumed\u001b[0m consumed.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Reasoning about Unmodelled Concepts- Incorporating Class Taxonomies in Probabilistic Relational Models \n",
      "A \u001b[41mtransferable\u001b[0m transferable problem in the application of first-order probabilistic methods is the enormous \u001b[41mfixed\u001b[0m fixed of graphical models they imply. The \u001b[41mfixed\u001b[0m fixed results from the possible worlds that can be generated by a domain of objects and relations. One of the reasons for this \u001b[41mabundance\u001b[0m abundance is that so far the approaches do not sufficiently exploit the \u001b[41massortative\u001b[0m assortative and \u001b[41msubspace\u001b[0m subspace of possible worlds in order to encode the models more compactly. We propose fuzzy inference in Markov logic networks, which enables the use of taxonomic \u001b[41msentence\u001b[0m sentence as a source of imposing \u001b[41massortative\u001b[0m assortative onto possible worlds. We show that by exploiting this \u001b[41massortative\u001b[0m assortative, probability distributions can be represented more compactly and that the reasoning systems become capable of reasoning about concepts not contained in the probabilistic \u001b[41msentence\u001b[0m sentence \u001b[41mspread\u001b[0m spread.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Reasoning about Unmodelled Concepts- Incorporating Class Taxonomies in Probabilistic Relational Models \n",
      "A \u001b[41mcapable\u001b[0m capable problem in the application of first-order probabilistic methods is the enormous \u001b[41mdecreased\u001b[0m decreased of graphical models they imply. The \u001b[41mdecreased\u001b[0m decreased results from the possible worlds that can be generated by a domain of objects and relations. One of the reasons for this explosion is that so far the approaches do not sufficiently exploit the \u001b[41mtemporal\u001b[0m temporal and \u001b[41mhilbert\u001b[0m hilbert of possible worlds in order to encode the models more compactly. We propose fuzzy \u001b[41moversampling\u001b[0m oversampling in Markov logic networks, which enables the use of taxonomic \u001b[41memotion\u001b[0m emotion as a source of imposing \u001b[41mtemporal\u001b[0m temporal onto possible worlds. We show that by exploiting this \u001b[41mtemporal\u001b[0m temporal, probability distributions can be represented more compactly and that the reasoning systems become capable of reasoning about concepts not contained in the probabilistic \u001b[41memotion\u001b[0m emotion \u001b[41mswitch\u001b[0m switch.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "An Approach to Find Missing Values in Medical Datasets \n",
      "\u001b[46mmining\u001b[0m medical datasets is a challenging problem before data \u001b[46mmining\u001b[0m researchers as these datasets have several hidden challenges compared to conventional datasets. Starting from the \u001b[46mcollection\u001b[0m of samples through field experiments and clinical trials to performing classification, there are numerous challenges at every \u001b[46mstage\u001b[0m in the \u001b[46mmining\u001b[0m \u001b[46mprocess\u001b[0m. The preprocessing phase in the \u001b[46mmining\u001b[0m \u001b[46mprocess\u001b[0m itself is a challenging \u001b[46missue\u001b[0m when, we work on medical datasets. One of the prime challenges in \u001b[46mmining\u001b[0m medical datasets is handling missing values which is part of preprocessing phase. In this paper, we address the \u001b[46missue\u001b[0m of handling missing values in medical dataset \u001b[46mconsisting\u001b[0m of categorical \u001b[46mattribute\u001b[0m values. The main contribution of this research is to use the proposed \u001b[46mimputation\u001b[0m \u001b[46mmeasure\u001b[0m to estimate and fix the missing values. We discuss a case study to demonstrate the \u001b[46mworking\u001b[0m of proposed \u001b[46mmeasure\u001b[0m.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "An Approach to Find Missing Values in Medical Datasets \n",
      "\u001b[41mverification\u001b[0m Verification medical datasets is a challenging problem before data \u001b[41mverification\u001b[0m verification researchers as these datasets have several hidden challenges compared to conventional datasets. Starting from the \u001b[41mmigration\u001b[0m migration of samples through field experiments and clinical trials to performing classification, there are numerous challenges at every stage in the \u001b[41mverification\u001b[0m verification \u001b[41minplane\u001b[0m inplane. The preprocessing \u001b[41mcodec\u001b[0m codec in the \u001b[41mverification\u001b[0m verification \u001b[41minplane\u001b[0m inplane itself is a challenging \u001b[41mprogress\u001b[0m progress when, we work on medical datasets. One of the prime challenges in \u001b[41mverification\u001b[0m verification medical datasets is handling missing values which is part of preprocessing \u001b[41mcodec\u001b[0m codec. In this paper, we address the \u001b[41mprogress\u001b[0m progress of handling missing values in medical dataset \u001b[41msegmentlevel\u001b[0m segmentlevel of categorical \u001b[41mlabeling\u001b[0m labeling values. The main contribution of this research is to use the proposed \u001b[41melimination\u001b[0m elimination \u001b[41mhypergeometric\u001b[0m hypergeometric to estimate and fix the missing values. We discuss a case study to demonstrate the \u001b[41mmanaging\u001b[0m managing of proposed \u001b[41mhypergeometric\u001b[0m hypergeometric.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "An Approach to Find Missing Values in Medical Datasets \n",
      "\u001b[41massisting\u001b[0m Assisting medical datasets is a challenging problem before data \u001b[41massisting\u001b[0m assisting researchers as these datasets have several hidden challenges compared to conventional datasets. Starting from the \u001b[41maccessibility\u001b[0m accessibility of samples through field experiments and clinical trials to performing classification, there are numerous challenges at every \u001b[41mpyramid\u001b[0m pyramid in the \u001b[41massisting\u001b[0m assisting process. The preprocessing \u001b[41mlowfrequency\u001b[0m lowfrequency in the \u001b[41massisting\u001b[0m assisting process itself is a challenging \u001b[41mavailability\u001b[0m availability when, we work on medical datasets. One of the prime challenges in \u001b[41massisting\u001b[0m assisting medical datasets is handling missing values which is part of preprocessing \u001b[41mlowfrequency\u001b[0m lowfrequency. In this paper, we address the \u001b[41mavailability\u001b[0m availability of handling missing values in medical dataset \u001b[41mwsj\u001b[0m wsj of categorical \u001b[41mmtl\u001b[0m mtl values. The main contribution of this research is to use the proposed \u001b[41mbinning\u001b[0m binning \u001b[41mquadrature\u001b[0m quadrature to estimate and fix the missing values. We discuss a case study to demonstrate the \u001b[41maccessibility\u001b[0m accessibility of proposed \u001b[41mquadrature\u001b[0m quadrature.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "An Approach to Find Missing Values in Medical Datasets \n",
      "\u001b[41mapplication\u001b[0m Application medical datasets is a challenging problem before data \u001b[41mapplication\u001b[0m application researchers as these datasets have several hidden challenges compared to conventional datasets. Starting from the \u001b[41marchitectural\u001b[0m architectural of samples through field experiments and clinical trials to performing classification, there are numerous challenges at every \u001b[41mrecurrent\u001b[0m recurrent in the \u001b[41mapplication\u001b[0m application \u001b[41mimaged\u001b[0m imaged. The preprocessing phase in the \u001b[41mapplication\u001b[0m application \u001b[41mimaged\u001b[0m imaged itself is a challenging \u001b[41mplethora\u001b[0m plethora when, we work on medical datasets. One of the prime challenges in \u001b[41mapplication\u001b[0m application medical datasets is handling missing values which is part of preprocessing phase. In this paper, we address the \u001b[41mplethora\u001b[0m plethora of handling missing values in medical dataset \u001b[41mfrav2d\u001b[0m frav2d of categorical \u001b[41mknn\u001b[0m knn values. The main contribution of this research is to use the proposed \u001b[41mreconstruction\u001b[0m reconstruction \u001b[41mintegervalued\u001b[0m integervalued to estimate and fix the missing values. We discuss a case study to demonstrate the \u001b[41mgis\u001b[0m gis of proposed \u001b[41mintegervalued\u001b[0m integervalued.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured \u001b[46mcorpora\u001b[0m \n",
      "We propose a framework to improve performance of distantly-supervised \u001b[46mrelation\u001b[0m \u001b[46mextraction\u001b[0m, by jointly learning to solve two related tasks: concept-instance \u001b[46mextraction\u001b[0m and \u001b[46mrelation\u001b[0m \u001b[46mextraction\u001b[0m. We combine this with a novel use of \u001b[46mdocument\u001b[0m structure: in some small, well-structured \u001b[46mcorpora\u001b[0m, sections can be identified that \u001b[46mcorrespond\u001b[0m to \u001b[46mrelation\u001b[0m arguments, and distantly-labeled examples from such sections tend to have good precision. Using these as seeds we extract additional \u001b[46mrelation\u001b[0m examples by applying label \u001b[46mpropagation\u001b[0m on a graph composed of noisy examples extracted from a large unstructured testing \u001b[46mcorpus\u001b[0m. Combined with the soft constraint that concept examples should have the same type as the second \u001b[46margument\u001b[0m of the \u001b[46mrelation\u001b[0m, we get significant improvements over several state-of-the-art approaches to distantly-supervised \u001b[46mrelation\u001b[0m \u001b[46mextraction\u001b[0m.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured \u001b[41mner\u001b[0m Ner \n",
      "We propose a framework to improve performance of distantly-supervised \u001b[41mextension\u001b[0m extension \u001b[41mderiving\u001b[0m deriving, by jointly learning to solve two related tasks: concept-instance \u001b[41mderiving\u001b[0m deriving and \u001b[41mextension\u001b[0m extension \u001b[41mderiving\u001b[0m deriving. We combine this with a novel use of \u001b[41mtopic\u001b[0m topic structure: in some small, well-structured \u001b[41mner\u001b[0m ner, sections can be identified that \u001b[41myield\u001b[0m yield to \u001b[41mextension\u001b[0m extension arguments, and distantly-labeled examples from such sections tend to have good \u001b[41mtop1\u001b[0m top1. Using these as seeds we extract additional \u001b[41mextension\u001b[0m extension examples by applying label \u001b[41msuppression\u001b[0m suppression on a graph composed of noisy examples extracted from a large unstructured testing corpus. Combined with the soft constraint that concept examples should have the same type as the second \u001b[41mintensional\u001b[0m intensional of the \u001b[41mextension\u001b[0m extension, we get significant improvements over several state-of-the-art approaches to distantly-supervised \u001b[41mextension\u001b[0m extension \u001b[41mderiving\u001b[0m deriving.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured \u001b[41mwordnet\u001b[0m Wordnet \n",
      "We propose a framework to improve performance of distantly-supervised \u001b[41mnondeterminism\u001b[0m nondeterminism \u001b[41mestimating\u001b[0m estimating, by jointly learning to solve two related tasks: concept-instance \u001b[41mestimating\u001b[0m estimating and \u001b[41mnondeterminism\u001b[0m nondeterminism \u001b[41mestimating\u001b[0m estimating. We combine this with a novel use of \u001b[41mtext\u001b[0m text structure: in some small, well-structured \u001b[41mwordnet\u001b[0m wordnet, sections can be identified that \u001b[41myield\u001b[0m yield to \u001b[41mnondeterminism\u001b[0m nondeterminism arguments, and distantly-labeled examples from such sections tend to have good \u001b[41mpercentage\u001b[0m percentage. Using these as seeds we extract additional \u001b[41mnondeterminism\u001b[0m nondeterminism examples by applying label propagation on a graph composed of noisy examples extracted from a large unstructured testing \u001b[41mtexts\u001b[0m texts. Combined with the soft constraint that concept examples should have the same type as the second \u001b[41mrewriting\u001b[0m rewriting of the \u001b[41mnondeterminism\u001b[0m nondeterminism, we get significant improvements over several state-of-the-art approaches to distantly-supervised \u001b[41mnondeterminism\u001b[0m nondeterminism \u001b[41mestimating\u001b[0m estimating.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured \u001b[41mword2vec\u001b[0m Word2vec \n",
      "We propose a framework to improve performance of distantly-supervised \u001b[41msemantics\u001b[0m semantics \u001b[41mderiving\u001b[0m deriving, by jointly learning to solve two related tasks: concept-instance \u001b[41mderiving\u001b[0m deriving and \u001b[41msemantics\u001b[0m semantics \u001b[41mderiving\u001b[0m deriving. We combine this with a novel use of \u001b[41mgrounding\u001b[0m grounding structure: in some small, well-structured \u001b[41mword2vec\u001b[0m word2vec, sections can be identified that \u001b[41mentail\u001b[0m entail to \u001b[41msemantics\u001b[0m semantics arguments, and distantly-labeled examples from such sections tend to have good precision. Using these as seeds we extract additional \u001b[41msemantics\u001b[0m semantics examples by applying label \u001b[41mcompression\u001b[0m compression on a graph composed of noisy examples extracted from a large unstructured testing \u001b[41mmonolingual\u001b[0m monolingual. Combined with the soft constraint that concept examples should have the same type as the second \u001b[41mintensional\u001b[0m intensional of the \u001b[41msemantics\u001b[0m semantics, we get significant improvements over several state-of-the-art approaches to distantly-supervised \u001b[41msemantics\u001b[0m semantics \u001b[41mderiving\u001b[0m deriving.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Service on Demand: \u001b[46mdrone\u001b[0m \u001b[46mbase\u001b[0m Stations Cruising in the Cellular Network \n",
      "In this paper, the \u001b[46mdeployment\u001b[0m of \u001b[46mdrone\u001b[0m \u001b[46mbase\u001b[0m stations to provide higher performance in the cellular networks is analyzed. In particular, we investigate a new \u001b[46mmobility\u001b[0m model for \u001b[46mdrone\u001b[0m \u001b[46mbase\u001b[0m stations where they can move freely in the network, ignoring the \u001b[46mcell\u001b[0m boundaries. Free movement model for drones bring out new challenges such as \u001b[46muser\u001b[0m \u001b[46massociation\u001b[0m and physical \u001b[46mcollision\u001b[0m among drones. We consider two \u001b[46muser\u001b[0m \u001b[46massociation\u001b[0m schemes and evaluate their performance through simulation. We show that by deploying a smart \u001b[46muser\u001b[0m \u001b[46massociation\u001b[0m scheme in the free movement model, the obtained results are greatly better than those restricting each \u001b[46mdrone\u001b[0m \u001b[46mbase\u001b[0m \u001b[46mstation\u001b[0m to fly over a certain small \u001b[46mcell\u001b[0m area, and serving local users. Additionally, the impact of drones'movement on the \u001b[46mload\u001b[0m \u001b[46mbalance\u001b[0m, signal \u001b[46mstrength\u001b[0m and interference are studied. Moreover, we show that our proposed algorithm can maintain a comfortable distance among the drones to avoid physical \u001b[46mcollision\u001b[0m.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Service on Demand: \u001b[41mlocomotion\u001b[0m Locomotion \u001b[41mtransmitting\u001b[0m Transmitting Stations Cruising in the Cellular Network \n",
      "In this paper, the \u001b[41mipv4\u001b[0m ipv4 of \u001b[41mlocomotion\u001b[0m locomotion \u001b[41mtransmitting\u001b[0m transmitting stations to provide higher performance in the cellular networks is analyzed. In particular, we investigate a new mobility model for \u001b[41mlocomotion\u001b[0m locomotion \u001b[41mtransmitting\u001b[0m transmitting stations where they can move freely in the network, ignoring the \u001b[41mlink\u001b[0m link boundaries. Free \u001b[41mmav\u001b[0m mav model for drones bring out new challenges such as \u001b[41mparticipant\u001b[0m participant \u001b[41mcontinual\u001b[0m continual and physical \u001b[41msafe\u001b[0m safe among drones. We consider two \u001b[41mparticipant\u001b[0m participant \u001b[41mcontinual\u001b[0m continual schemes and evaluate their performance through simulation. We show that by deploying a smart \u001b[41mparticipant\u001b[0m participant \u001b[41mcontinual\u001b[0m continual scheme in the free \u001b[41mmav\u001b[0m mav model, the obtained results are greatly better than those restricting each \u001b[41mlocomotion\u001b[0m locomotion \u001b[41mtransmitting\u001b[0m transmitting \u001b[41mreceivers\u001b[0m receivers to fly over a certain small \u001b[41mlink\u001b[0m link area, and serving local users. Additionally, the impact of drones'\u001b[41mmav\u001b[0m mav on the \u001b[41mbalancing\u001b[0m balancing \u001b[41mworst\u001b[0m worst, signal \u001b[41masymmetry\u001b[0m asymmetry and interference are studied. Moreover, we show that our proposed algorithm can maintain a comfortable distance among the drones to avoid physical \u001b[41msafe\u001b[0m safe.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Service on Demand: \u001b[41mflying\u001b[0m Flying \u001b[41msynchronized\u001b[0m Synchronized Stations Cruising in the Cellular Network \n",
      "In this paper, the \u001b[41mquic\u001b[0m quic of \u001b[41mflying\u001b[0m flying \u001b[41msynchronized\u001b[0m synchronized stations to provide higher performance in the cellular networks is analyzed. In particular, we investigate a new \u001b[41mqueue\u001b[0m queue model for \u001b[41mflying\u001b[0m flying \u001b[41msynchronized\u001b[0m synchronized stations where they can move freely in the network, ignoring the \u001b[41mrelay\u001b[0m relay boundaries. Free movement model for drones bring out new challenges such as \u001b[41mauthority\u001b[0m authority \u001b[41mcontrastive\u001b[0m contrastive and physical \u001b[41mleg\u001b[0m leg among drones. We consider two \u001b[41mauthority\u001b[0m authority \u001b[41mcontrastive\u001b[0m contrastive schemes and evaluate their performance through simulation. We show that by deploying a smart \u001b[41mauthority\u001b[0m authority \u001b[41mcontrastive\u001b[0m contrastive scheme in the free movement model, the obtained results are greatly better than those restricting each \u001b[41mflying\u001b[0m flying \u001b[41msynchronized\u001b[0m synchronized \u001b[41mreception\u001b[0m reception to fly over a certain small \u001b[41mrelay\u001b[0m relay area, and serving local users. Additionally, the impact of drones'movement on the \u001b[41mbattery\u001b[0m battery \u001b[41mthresholds\u001b[0m thresholds, signal \u001b[41mcurse\u001b[0m curse and interference are studied. Moreover, we show that our proposed algorithm can maintain a comfortable distance among the drones to avoid physical \u001b[41mleg\u001b[0m leg.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Service on Demand: \u001b[41mmanipulator\u001b[0m Manipulator \u001b[41mcommunication\u001b[0m Communication Stations Cruising in the Cellular Network \n",
      "In this paper, the \u001b[41mquic\u001b[0m quic of \u001b[41mmanipulator\u001b[0m manipulator \u001b[41mcommunication\u001b[0m communication stations to provide higher performance in the cellular networks is analyzed. In particular, we investigate a new \u001b[41mmulticast\u001b[0m multicast model for \u001b[41mmanipulator\u001b[0m manipulator \u001b[41mcommunication\u001b[0m communication stations where they can move freely in the network, ignoring the \u001b[41mmacrocell\u001b[0m macrocell boundaries. Free movement model for drones bring out new challenges such as \u001b[41mclient\u001b[0m client \u001b[41msvms\u001b[0m svms and physical \u001b[41mposition\u001b[0m position among drones. We consider two \u001b[41mclient\u001b[0m client \u001b[41msvms\u001b[0m svms schemes and evaluate their performance through simulation. We show that by deploying a smart \u001b[41mclient\u001b[0m client \u001b[41msvms\u001b[0m svms scheme in the free movement model, the obtained results are greatly better than those restricting each \u001b[41mmanipulator\u001b[0m manipulator \u001b[41mcommunication\u001b[0m communication \u001b[41mhop\u001b[0m hop to fly over a certain small \u001b[41mmacrocell\u001b[0m macrocell area, and serving local users. Additionally, the impact of drones'movement on the \u001b[41mcaching\u001b[0m caching \u001b[41mrelative\u001b[0m relative, signal \u001b[41mparadox\u001b[0m paradox and interference are studied. Moreover, we show that our proposed algorithm can maintain a comfortable distance among the drones to avoid physical \u001b[41mposition\u001b[0m position.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "On Generation of Adversarial Examples using \u001b[46mconvex\u001b[0m \u001b[46mprogramming\u001b[0m \n",
      "It has been observed that deep learning architectures tend to make erroneous decisions with high \u001b[46mreliability\u001b[0m for particularly designed adversarial instances. In this work, we show that the \u001b[46mperturbation\u001b[0m analysis of these architectures provides a \u001b[46mframework\u001b[0m for generating adversarial instances by \u001b[46mconvex\u001b[0m \u001b[46mprogramming\u001b[0m which, for classification tasks, is able to recover variants of existing non-adaptive adversarial methods. The proposed \u001b[46mframework\u001b[0m can be used for the design of adversarial \u001b[46mnoise\u001b[0m under various desirable constraints and different types of networks. Moreover, this \u001b[46mframework\u001b[0m is capable of explaining various existing adversarial methods and can be used to derive new algorithms as well. We make use of these results to obtain novel algorithms. The experiments show the competitive performance of the obtained solutions, in terms of fooling \u001b[46mratio\u001b[0m, when benchmarked with well-known adversarial methods.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "On Generation of Adversarial Examples using \u001b[41mmomentum\u001b[0m Momentum \u001b[41mgui\u001b[0m Gui \n",
      "It has been observed that deep learning architectures tend to make erroneous decisions with high \u001b[41mbackoff\u001b[0m backoff for particularly designed adversarial instances. In this work, we show that the \u001b[41msgd\u001b[0m sgd analysis of these architectures provides a framework for generating adversarial instances by \u001b[41mmomentum\u001b[0m momentum \u001b[41mgui\u001b[0m gui which, for classification tasks, is able to recover variants of existing non-adaptive adversarial methods. The proposed framework can be used for the design of adversarial \u001b[41mblind\u001b[0m blind under various desirable constraints and different types of networks. Moreover, this framework is capable of explaining various existing adversarial methods and can be used to derive new \u001b[41mviews\u001b[0m views as well. We make use of these results to obtain novel \u001b[41mviews\u001b[0m views. The experiments show the competitive performance of the obtained solutions, in terms of fooling \u001b[41mtime\u001b[0m time, when benchmarked with well-known adversarial methods.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "On Generation of Adversarial Examples using \u001b[41mnorm\u001b[0m Norm \u001b[41mportal\u001b[0m Portal \n",
      "It has been observed that deep learning architectures tend to make erroneous decisions with high \u001b[41mbackhaul\u001b[0m backhaul for particularly designed adversarial instances. In this work, we show that the \u001b[41mhessian\u001b[0m hessian analysis of these architectures provides a \u001b[41malgorithm\u001b[0m algorithm for generating adversarial instances by \u001b[41mnorm\u001b[0m norm \u001b[41mportal\u001b[0m portal which, for classification tasks, is able to recover variants of existing non-adaptive adversarial methods. The proposed \u001b[41malgorithm\u001b[0m algorithm can be used for the design of adversarial \u001b[41mmicrophone\u001b[0m microphone under various desirable constraints and different types of networks. Moreover, this \u001b[41malgorithm\u001b[0m algorithm is capable of explaining various existing adversarial methods and can be used to derive new \u001b[41mviews\u001b[0m views as well. We make use of these results to obtain novel \u001b[41mviews\u001b[0m views. The experiments show the competitive performance of the obtained solutions, in terms of fooling ratio, when benchmarked with well-known adversarial methods.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "On Generation of Adversarial Examples using \u001b[41mminimization\u001b[0m Minimization \u001b[41mengine\u001b[0m Engine \n",
      "It has been observed that deep learning architectures tend to make erroneous decisions with high \u001b[41mbuffering\u001b[0m buffering for particularly designed adversarial instances. In this work, we show that the \u001b[41moptimization\u001b[0m optimization analysis of these architectures provides a \u001b[41mestimator\u001b[0m estimator for generating adversarial instances by \u001b[41mminimization\u001b[0m minimization \u001b[41mengine\u001b[0m engine which, for classification tasks, is able to recover variants of existing non-adaptive adversarial methods. The proposed \u001b[41mestimator\u001b[0m estimator can be used for the design of adversarial \u001b[41mcodec\u001b[0m codec under various desirable constraints and different types of networks. Moreover, this \u001b[41mestimator\u001b[0m estimator is capable of explaining various existing adversarial methods and can be used to derive new algorithms as well. We make use of these results to obtain novel algorithms. The experiments show the competitive performance of the obtained solutions, in terms of fooling \u001b[41mon2\u001b[0m on2, when benchmarked with well-known adversarial methods.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "A Framework for \u001b[46munderstanding\u001b[0m Unintended Consequences of \u001b[46mmachine\u001b[0m Learning \n",
      "As \u001b[46mmachine\u001b[0m learning increasingly affects people and \u001b[46msociety\u001b[0m, it is important that we strive for a comprehensive and unified \u001b[46munderstanding\u001b[0m of how and why unwanted consequences arise. For instance, \u001b[46mdownstream\u001b[0m harms to particular groups are often blamed on `` biased data, '' but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of \u001b[46mdownstream\u001b[0m \u001b[46mharm\u001b[0m in \u001b[46mmachine\u001b[0m learning into five distinct categories spanning the data \u001b[46mgeneration\u001b[0m and \u001b[46mmachine\u001b[0m learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an \u001b[46munderstanding\u001b[0m of application-specific populations and data \u001b[46mgeneration\u001b[0m processes, rather than relying on general claims about what may or may not be `` fair. ''\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "A Framework for \u001b[41mpractice\u001b[0m Practice Unintended Consequences of \u001b[41mfuture\u001b[0m Future Learning \n",
      "As \u001b[41mfuture\u001b[0m future learning increasingly affects people and society, it is important that we strive for a comprehensive and unified \u001b[41mpractice\u001b[0m practice of how and why unwanted consequences arise. For instance, \u001b[41mtacotron\u001b[0m tacotron harms to particular groups are often blamed on `` biased data, '' but this concept \u001b[41minitiate\u001b[0m initiate too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of \u001b[41mtacotron\u001b[0m tacotron \u001b[41mdown\u001b[0m down in \u001b[41mfuture\u001b[0m future learning into five distinct categories spanning the data \u001b[41mwarehouse\u001b[0m warehouse and \u001b[41mfuture\u001b[0m future learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an \u001b[41mpractice\u001b[0m practice of application-specific populations and data \u001b[41mwarehouse\u001b[0m warehouse processes, rather than relying on general claims about what may or may not be `` fair. ''\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "A Framework for \u001b[41mexpertise\u001b[0m Expertise Unintended Consequences of \u001b[41mwellbeing\u001b[0m Wellbeing Learning \n",
      "As \u001b[41mwellbeing\u001b[0m wellbeing learning increasingly affects people and society, it is important that we strive for a comprehensive and unified \u001b[41mexpertise\u001b[0m expertise of how and why unwanted consequences arise. For instance, \u001b[41mlistening\u001b[0m listening harms to particular groups are often blamed on `` biased data, '' but this concept \u001b[41mtailor\u001b[0m tailor too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of \u001b[41mlistening\u001b[0m listening \u001b[41mreturn\u001b[0m return in \u001b[41mwellbeing\u001b[0m wellbeing learning into five distinct categories spanning the data \u001b[41magile\u001b[0m agile and \u001b[41mwellbeing\u001b[0m wellbeing learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an \u001b[41mexpertise\u001b[0m expertise of application-specific populations and data \u001b[41magile\u001b[0m agile processes, rather than relying on general claims about what may or may not be `` fair. ''\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "A Framework for \u001b[41mbiology\u001b[0m Biology Unintended Consequences of \u001b[41mhumanmachine\u001b[0m Humanmachine Learning \n",
      "As \u001b[41mhumanmachine\u001b[0m humanmachine learning increasingly affects people and \u001b[41mgaming\u001b[0m gaming, it is important that we strive for a comprehensive and unified \u001b[41mbiology\u001b[0m biology of how and why unwanted consequences arise. For instance, \u001b[41morthography\u001b[0m orthography harms to particular groups are often blamed on `` biased data, '' but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of \u001b[41morthography\u001b[0m orthography \u001b[41mplay\u001b[0m play in \u001b[41mhumanmachine\u001b[0m humanmachine learning into five distinct categories spanning the data \u001b[41mforensic\u001b[0m forensic and \u001b[41mhumanmachine\u001b[0m humanmachine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an \u001b[41mbiology\u001b[0m biology of application-specific populations and data \u001b[41mforensic\u001b[0m forensic processes, rather than relying on general claims about what may or may not be `` fair. ''\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Model \u001b[46mcomparison\u001b[0m of Dark \u001b[46menergy\u001b[0m models Using Deep \u001b[46mnetwork\u001b[0m \n",
      "This work uses the combination of the variational auto-encoder and the generative adversarial \u001b[46mnetwork\u001b[0m to compare different dark \u001b[46menergy\u001b[0m models in the \u001b[46mlight\u001b[0m of the observations, e.g., the distance \u001b[46mmodulus\u001b[0m from SNIa. The \u001b[46mnetwork\u001b[0m finds the analytical variational approximation to the true \u001b[46mposterior\u001b[0m of the latent parameters of the models, yielding consistent model \u001b[46mcomparison\u001b[0m results to those derived by the standard \u001b[46mbayesian\u001b[0m method which suffers from the computationally expensive integral over the parameters in the \u001b[46mproduct\u001b[0m of the \u001b[46mlikelihood\u001b[0m and the prior. The parallel computation nature of the \u001b[46mnetwork\u001b[0m together with the stochastic \u001b[46mgradient\u001b[0m \u001b[46mdescent\u001b[0m optimization technique lead to an efficient way to \u001b[46mcomparison\u001b[0m the physical models given a set of observations. The converged \u001b[46mnetwork\u001b[0m also provides \u001b[46minterpolation\u001b[0m to dataset which is useful for data reconstruction.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Model \u001b[41mfashionmnist\u001b[0m Fashionmnist of Dark \u001b[41mqualityofservice\u001b[0m Qualityofservice models Using Deep \u001b[41mrcnn\u001b[0m Rcnn \n",
      "This work uses the combination of the variational auto-encoder and the generative adversarial \u001b[41mrcnn\u001b[0m rcnn to compare different dark \u001b[41mqualityofservice\u001b[0m qualityofservice models in the \u001b[41mselfsimilar\u001b[0m selfsimilar of the observations, e.g., the distance \u001b[41mparametrization\u001b[0m parametrization from SNIa. The \u001b[41mrcnn\u001b[0m rcnn finds the analytical variational approximation to the true \u001b[41msubgradient\u001b[0m subgradient of the latent parameters of the models, yielding consistent model \u001b[41mfashionmnist\u001b[0m fashionmnist results to those derived by the standard \u001b[41mmathematical\u001b[0m Mathematical method which suffers from the computationally expensive integral over the parameters in the \u001b[41mhilbertschmidt\u001b[0m hilbertschmidt of the \u001b[41mapproximate\u001b[0m approximate and the prior. The parallel computation nature of the \u001b[41mrcnn\u001b[0m rcnn together with the stochastic \u001b[41mvariance\u001b[0m variance \u001b[41msaddle\u001b[0m saddle optimization technique \u001b[41mmotivate\u001b[0m motivate to an efficient way to \u001b[41mfashionmnist\u001b[0m fashionmnist the physical models given a set of observations. The converged \u001b[41mrcnn\u001b[0m rcnn also provides interpolation to dataset which is useful for data reconstruction.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Model \u001b[41mheldout\u001b[0m Heldout of Dark \u001b[41mmulticast\u001b[0m Multicast models Using Deep \u001b[41mmlp\u001b[0m Mlp \n",
      "This work uses the combination of the variational auto-encoder and the generative adversarial \u001b[41mmlp\u001b[0m mlp to compare different dark \u001b[41mmulticast\u001b[0m multicast models in the \u001b[41mpolycephalum\u001b[0m polycephalum of the observations, e.g., the distance \u001b[41mspline\u001b[0m spline from SNIa. The \u001b[41mmlp\u001b[0m mlp finds the analytical variational approximation to the true \u001b[41msmoothness\u001b[0m smoothness of the latent parameters of the models, yielding consistent model \u001b[41mheldout\u001b[0m heldout results to those derived by the standard \u001b[41mgeneralized\u001b[0m Generalized method which suffers from the computationally expensive integral over the parameters in the \u001b[41msupremum\u001b[0m supremum of the \u001b[41msaddle\u001b[0m saddle and the prior. The parallel computation nature of the \u001b[41mmlp\u001b[0m mlp together with the stochastic \u001b[41mweight\u001b[0m weight descent optimization technique \u001b[41menable\u001b[0m enable to an efficient way to \u001b[41mheldout\u001b[0m heldout the physical models given a set of observations. The converged \u001b[41mmlp\u001b[0m mlp also provides \u001b[41mbinning\u001b[0m binning to dataset which is useful for data reconstruction.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Model \u001b[41mstanford\u001b[0m Stanford of Dark \u001b[41mallocation\u001b[0m Allocation models Using Deep \u001b[41mtreestructured\u001b[0m Treestructured \n",
      "This work uses the combination of the variational auto-encoder and the generative adversarial \u001b[41mtreestructured\u001b[0m treestructured to compare different dark \u001b[41mallocation\u001b[0m allocation models in the \u001b[41mink\u001b[0m ink of the observations, e.g., the distance \u001b[41mtranscendental\u001b[0m transcendental from SNIa. The \u001b[41mtreestructured\u001b[0m treestructured finds the analytical variational approximation to the true \u001b[41mwasserstein\u001b[0m wasserstein of the latent parameters of the models, yielding consistent model \u001b[41mstanford\u001b[0m stanford results to those derived by the standard \u001b[41manalytical\u001b[0m Analytical method which suffers from the computationally expensive integral over the parameters in the \u001b[41mtaylor\u001b[0m taylor of the \u001b[41mconvexity\u001b[0m convexity and the prior. The parallel computation nature of the \u001b[41mtreestructured\u001b[0m treestructured together with the stochastic \u001b[41mconvergence\u001b[0m convergence \u001b[41mobjective\u001b[0m objective optimization technique lead to an efficient way to \u001b[41mstanford\u001b[0m stanford the physical models given a set of observations. The converged \u001b[41mtreestructured\u001b[0m treestructured also provides \u001b[41mclipping\u001b[0m clipping to dataset which is useful for data reconstruction.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Solving the apparent diversity-accuracy \u001b[46mdilemma\u001b[0m of \u001b[46mrecommender\u001b[0m systems \n",
      "\u001b[46mrecommender\u001b[0m systems use data on past \u001b[46muser\u001b[0m preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse \u001b[46mniche\u001b[0m objects, the most reliably accurate results are obtained by methods that recommend objects based on \u001b[46muser\u001b[0m or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of \u001b[46mdiversity\u001b[0m and show how it can be used to resolve this apparent \u001b[46mdilemma\u001b[0m when combined in an elegant \u001b[46mhybrid\u001b[0m with an accuracy-focused algorithm. By tuning the \u001b[46mhybrid\u001b[0m appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and \u001b[46mdiversity\u001b[0m of recommendations.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Solving the apparent diversity-accuracy \u001b[41mdiscriminability\u001b[0m discriminability of recommender systems \n",
      "Recommender systems use data on past \u001b[41mdesigner\u001b[0m designer preferences to predict possible future likes and interests. A key \u001b[41mwidespread\u001b[0m widespread is that while the most useful individual recommendations are to be found among diverse \u001b[41mgeography\u001b[0m geography objects, the most reliably accurate results are obtained by methods that recommend objects based on \u001b[41mdesigner\u001b[0m designer or object similarity. In this paper we introduce a new algorithm specifically to address the \u001b[41mwidespread\u001b[0m widespread of \u001b[41mstarvation\u001b[0m starvation and show how it can be used to resolve this apparent \u001b[41mdiscriminability\u001b[0m discriminability when combined in an elegant \u001b[41mgp\u001b[0m gp with an accuracy-focused algorithm. By tuning the \u001b[41mgp\u001b[0m gp appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and \u001b[41mstarvation\u001b[0m starvation of recommendations.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Solving the apparent diversity-accuracy dilemma of \u001b[41mtweet\u001b[0m tweet systems \n",
      "\u001b[41mtweet\u001b[0m Tweet systems use data on past \u001b[41madvice\u001b[0m advice preferences to predict possible future likes and interests. A key \u001b[41mabundance\u001b[0m abundance is that while the most useful individual recommendations are to be found among diverse \u001b[41mcurricula\u001b[0m curricula objects, the most reliably accurate results are obtained by methods that recommend objects based on \u001b[41madvice\u001b[0m advice or object similarity. In this paper we introduce a new algorithm specifically to address the \u001b[41mabundance\u001b[0m abundance of \u001b[41mgaps\u001b[0m gaps and show how it can be used to resolve this apparent dilemma when combined in an elegant \u001b[41minplace\u001b[0m inplace with an accuracy-focused algorithm. By tuning the \u001b[41minplace\u001b[0m inplace appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and \u001b[41mgaps\u001b[0m gaps of recommendations.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Solving the apparent diversity-accuracy \u001b[41minterplay\u001b[0m interplay of \u001b[41mmendeley\u001b[0m mendeley systems \n",
      "\u001b[41mmendeley\u001b[0m Mendeley systems use data on past \u001b[41manalyst\u001b[0m analyst preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse \u001b[41mparks\u001b[0m parks objects, the most reliably accurate results are obtained by methods that recommend objects based on \u001b[41manalyst\u001b[0m analyst or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of \u001b[41mpropensity\u001b[0m propensity and show how it can be used to resolve this apparent \u001b[41minterplay\u001b[0m interplay when combined in an elegant \u001b[41mmultistep\u001b[0m multistep with an accuracy-focused algorithm. By tuning the \u001b[41mmultistep\u001b[0m multistep appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and \u001b[41mpropensity\u001b[0m propensity of recommendations.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "Learning Restricted Regular Expressions with \u001b[46minterleaving\u001b[0m \n",
      "The advantages for the presence of an XML \u001b[46mschema\u001b[0m for XML documents are numerous. However, many XML documents in practice are not accompanied by a \u001b[46mschema\u001b[0m or by a valid \u001b[46mschema\u001b[0m. \u001b[46mrelax\u001b[0m NG is a popular and powerful \u001b[46mschema\u001b[0m language, which supports the unconstrained \u001b[46minterleaving\u001b[0m operator. Focusing on the inference of \u001b[46mrelax\u001b[0m NG, we propose a new \u001b[46msubclass\u001b[0m of regular expressions with \u001b[46minterleaving\u001b[0m and design a polynomial inference algorithm. Then we conducted a series of experiments based on large-scale real data and on three XML data \u001b[46mcorpora\u001b[0m, and experimental results show that our \u001b[46msubclass\u001b[0m has a better \u001b[46mpracticality\u001b[0m than previous ones, and the regular expressions inferred by our algorithm are more precise.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "Learning Restricted Regular Expressions with Interleaving \n",
      "The advantages for the presence of an XML \u001b[41mmatlab\u001b[0m matlab for XML documents are numerous. However, many XML documents in practice are not accompanied by a \u001b[41mmatlab\u001b[0m matlab or by a valid \u001b[41mmatlab\u001b[0m matlab. \u001b[41mgeneralize\u001b[0m Generalize NG is a popular and powerful \u001b[41mmatlab\u001b[0m matlab language, which supports the unconstrained interleaving operator. Focusing on the \u001b[41msr\u001b[0m sr of \u001b[41mgeneralize\u001b[0m Generalize NG, we propose a new \u001b[41mequality\u001b[0m equality of regular expressions with interleaving and design a polynomial \u001b[41msr\u001b[0m sr algorithm. Then we conducted a series of experiments based on large-scale real data and on three XML data \u001b[41mlexicon\u001b[0m lexicon, and experimental results show that our \u001b[41mequality\u001b[0m equality has a better \u001b[41mability\u001b[0m ability than previous ones, and the regular expressions inferred by our algorithm are more precise.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "Learning Restricted Regular Expressions with \u001b[41mdags\u001b[0m Dags \n",
      "The advantages for the presence of an XML \u001b[41mimperative\u001b[0m imperative for XML documents are numerous. However, many XML documents in practice are not accompanied by a \u001b[41mimperative\u001b[0m imperative or by a valid \u001b[41mimperative\u001b[0m imperative. \u001b[41mperform\u001b[0m Perform NG is a popular and powerful \u001b[41mimperative\u001b[0m imperative language, which supports the unconstrained \u001b[41mdags\u001b[0m dags operator. Focusing on the \u001b[41mconversion\u001b[0m conversion of \u001b[41mperform\u001b[0m Perform NG, we propose a new \u001b[41madmissible\u001b[0m admissible of regular expressions with \u001b[41mdags\u001b[0m dags and design a polynomial \u001b[41mconversion\u001b[0m conversion algorithm. Then we conducted a series of experiments based on large-scale real data and on three XML data corpora, and experimental results show that our \u001b[41madmissible\u001b[0m admissible has a better \u001b[41musefulness\u001b[0m usefulness than previous ones, and the regular expressions inferred by our algorithm are more precise.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "Learning Restricted Regular Expressions with \u001b[41mrdfs\u001b[0m Rdfs \n",
      "The advantages for the presence of an XML \u001b[41mhaskell\u001b[0m haskell for XML documents are numerous. However, many XML documents in practice are not accompanied by a \u001b[41mhaskell\u001b[0m haskell or by a valid \u001b[41mhaskell\u001b[0m haskell. \u001b[41mvisualize\u001b[0m Visualize NG is a popular and powerful \u001b[41mhaskell\u001b[0m haskell language, which supports the unconstrained \u001b[41mrdfs\u001b[0m rdfs operator. Focusing on the inference of \u001b[41mvisualize\u001b[0m Visualize NG, we propose a new \u001b[41marbitrary\u001b[0m arbitrary of regular expressions with \u001b[41mrdfs\u001b[0m rdfs and design a polynomial inference algorithm. Then we conducted a series of experiments based on large-scale real data and on three XML data \u001b[41mngrams\u001b[0m ngrams, and experimental results show that our \u001b[41marbitrary\u001b[0m arbitrary has a better \u001b[41mpracticability\u001b[0m practicability than previous ones, and the regular expressions inferred by our algorithm are more precise.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    ORIGINAL\n",
      "A Preliminary \u001b[46mstudy\u001b[0m of Neural Network-based \u001b[46mapproximation\u001b[0m for \u001b[46mhpc\u001b[0m Applications \n",
      "\u001b[46mmachine\u001b[0m \u001b[46mlearning\u001b[0m, as a tool to learn and model complicated (\u001b[46mnon\u001b[0m) linear relationships between input and \u001b[46moutput\u001b[0m data sets, has shown preliminary \u001b[46msuccess\u001b[0m in some \u001b[46mhpc\u001b[0m problems. Using \u001b[46mmachine\u001b[0m \u001b[46mlearning\u001b[0m, scientists are able to augment existing simulations by improving accuracy and significantly reducing latencies. Our ongoing research work is to create a general framework to apply neural network-based models to \u001b[46mhpc\u001b[0m applications. In particular, we want to use the neural network to approximate and replace code regions within the \u001b[46mhpc\u001b[0m \u001b[46mapplication\u001b[0m to improve performance (i.e., reducing the \u001b[46mexecution\u001b[0m time) of the \u001b[46mhpc\u001b[0m \u001b[46mapplication\u001b[0m. In this paper, we present our preliminary \u001b[46mstudy\u001b[0m and results. Using two applications (the Newton-Raphson method and the Lennard-Jones (\u001b[46mlj\u001b[0m) potential in LAMMP) for our case \u001b[46mstudy\u001b[0m, we achieve up to 2.7x and 2.46x \u001b[46mspeedup\u001b[0m, respectively.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 0\n",
      "A Preliminary \u001b[41mdescribe\u001b[0m Describe of Neural Network-based \u001b[41ml1norm\u001b[0m L1norm for HPC Applications \n",
      "\u001b[41mliving\u001b[0m Living \u001b[41moneshot\u001b[0m oneshot, as a tool to learn and model complicated (\u001b[41mequivariance\u001b[0m equivariance) linear relationships between input and \u001b[41msfm\u001b[0m sfm data sets, has shown preliminary \u001b[41mlack\u001b[0m lack in some HPC problems. Using \u001b[41mliving\u001b[0m living \u001b[41moneshot\u001b[0m oneshot, scientists are able to augment existing simulations by improving accuracy and significantly reducing latencies. Our ongoing research work is to create a general framework to apply neural network-based models to HPC applications. In particular, we want to use the neural network to approximate and replace code regions within the HPC \u001b[41mdocumenting\u001b[0m documenting to improve performance (i.e., reducing the \u001b[41mportability\u001b[0m portability time) of the HPC \u001b[41mdocumenting\u001b[0m documenting. In this paper, we present our preliminary \u001b[41mdescribe\u001b[0m describe and results. Using two applications (the Newton-Raphson method and the Lennard-Jones (\u001b[41mzoomoutandin\u001b[0m Zoomoutandin) \u001b[41mflexibility\u001b[0m flexibility in LAMMP) for our case \u001b[41mdescribe\u001b[0m describe, we achieve up to 2.7x and 2.46x \u001b[41msaving\u001b[0m saving, respectively.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 1\n",
      "A Preliminary \u001b[41mdefine\u001b[0m Define of Neural Network-based \u001b[41msurrogate\u001b[0m Surrogate for \u001b[41mos\u001b[0m Os Applications \n",
      "\u001b[41mphilosophy\u001b[0m Philosophy learning, as a tool to learn and model complicated (\u001b[41mhermite\u001b[0m hermite) linear relationships between input and \u001b[41mminutiae\u001b[0m minutiae data sets, has shown preliminary \u001b[41mbreakthroughs\u001b[0m breakthroughs in some \u001b[41mos\u001b[0m Os problems. Using \u001b[41mphilosophy\u001b[0m philosophy learning, scientists are able to augment existing simulations by improving accuracy and significantly reducing latencies. Our ongoing research work is to create a general framework to apply neural network-based models to \u001b[41mos\u001b[0m Os applications. In particular, we want to use the neural network to approximate and replace code regions within the \u001b[41mos\u001b[0m Os \u001b[41mease\u001b[0m ease to improve performance (i.e., reducing the \u001b[41mfloatingpoint\u001b[0m floatingpoint time) of the \u001b[41mos\u001b[0m Os \u001b[41mease\u001b[0m ease. In this paper, we present our preliminary \u001b[41mdefine\u001b[0m define and results. Using two applications (the Newton-Raphson method and the Lennard-Jones (\u001b[41mcameraman\u001b[0m Cameraman) \u001b[41mcorrectness\u001b[0m correctness in LAMMP) for our case \u001b[41mdefine\u001b[0m define, we achieve up to 2.7x and 2.46x \u001b[41mimprovement\u001b[0m improvement, respectively.\n",
      "--------------------------------------------------------------------------------\n",
      "                                    FAKE 2\n",
      "A Preliminary \u001b[41mcompare\u001b[0m Compare of Neural Network-based \u001b[41mnonconvex\u001b[0m Nonconvex for \u001b[41mmulticore\u001b[0m Multicore Applications \n",
      "\u001b[41mhealth\u001b[0m Health \u001b[41mknn\u001b[0m knn, as a tool to learn and model complicated (\u001b[41mvectorvalued\u001b[0m vectorvalued) linear relationships between input and \u001b[41mmidlevel\u001b[0m midlevel data sets, has shown preliminary \u001b[41mintense\u001b[0m intense in some \u001b[41mmulticore\u001b[0m Multicore problems. Using \u001b[41mhealth\u001b[0m health \u001b[41mknn\u001b[0m knn, scientists are able to augment existing simulations by improving accuracy and significantly reducing latencies. Our ongoing research work is to create a general framework to apply neural network-based models to \u001b[41mmulticore\u001b[0m Multicore applications. In particular, we want to use the neural network to approximate and replace code regions within the \u001b[41mmulticore\u001b[0m Multicore \u001b[41mpreparation\u001b[0m preparation to improve performance (i.e., reducing the \u001b[41mio\u001b[0m io time) of the \u001b[41mmulticore\u001b[0m Multicore \u001b[41mpreparation\u001b[0m preparation. In this paper, we present our preliminary \u001b[41mcompare\u001b[0m compare and results. Using two applications (the Newton-Raphson method and the Lennard-Jones (\u001b[41mqalb\u001b[0m Qalb) potential in LAMMP) for our case \u001b[41mcompare\u001b[0m compare, we achieve up to 2.7x and 2.46x \u001b[41mimprovement\u001b[0m improvement, respectively.\n"
     ]
    }
   ],
   "source": [
    "for idx, ori_file in enumerate(original_docs):\n",
    "    print('-' * 80)\n",
    "    print(' ' * 36 + 'ORIGINAL')\n",
    "    print(ori_file)\n",
    "    for fake_idx in range(3 * (idx), 3 * (idx+1)):\n",
    "        print('-' * 80)\n",
    "        print(' ' * 36 + 'FAKE ' + str(fake_idx%3))\n",
    "        fake = generate_docs[fake_idx]\n",
    "        print(fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
